{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527c06ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:18:52.044240Z",
     "iopub.status.busy": "2024-06-14T20:18:52.043594Z",
     "iopub.status.idle": "2024-06-14T20:19:44.303648Z",
     "shell.execute_reply": "2024-06-14T20:19:44.302782Z"
    },
    "papermill": {
     "duration": 52.271287,
     "end_time": "2024-06-14T20:19:44.306162",
     "exception": false,
     "start_time": "2024-06-14T20:18:52.034875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle\n",
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Collecting mlflow\r\n",
      "  Downloading mlflow-2.13.2-py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.0.3)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.13.1)\r\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow)\r\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (7.0.0)\r\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4)\r\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.41)\r\n",
      "Collecting graphene<4 (from mlflow)\r\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.11.0)\r\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.5.2)\r\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.7.5)\r\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.26.4)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\r\n",
      "Requirement already satisfied: packaging<25 in /opt/conda/lib/python3.10/site-packages (from mlflow) (21.3)\r\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\r\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.20.3)\r\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (14.0.2)\r\n",
      "Requirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2023.3.post1)\r\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.0.1)\r\n",
      "Collecting querystring-parser<2 (from mlflow)\r\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\r\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.32.3)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.2)\r\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.11.4)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.25)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4.4)\r\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.2)\r\n",
      "Collecting gunicorn<23 (from mlflow)\r\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.9.0)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.18)\r\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.3)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\r\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\r\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.43b0)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.2.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.14.1)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\r\n",
      "Downloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\r\n",
      "Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\r\n",
      "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\n",
      "Installing collected packages: aniso8601, querystring-parser, graphql-core, cachetools, gunicorn, graphql-relay, graphene, mlflow\r\n",
      "  Attempting uninstall: cachetools\r\n",
      "    Found existing installation: cachetools 4.2.4\r\n",
      "    Uninstalling cachetools-4.2.4:\r\n",
      "      Successfully uninstalled cachetools-4.2.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 24.4.1 requires cubinlinker, which is not installed.\r\n",
      "cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 24.4.1 requires ptxcompiler, which is not installed.\r\n",
      "cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed aniso8601-9.0.1 cachetools-5.3.2 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 mlflow-2.13.2 querystring-parser-1.2.4\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Determine the environment and import preprocessing module accordingly\n",
    "def is_kaggle():\n",
    "    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if is_kaggle():\n",
    "    print(\"Running on Kaggle\")\n",
    "    # Assuming 'preprocessing.py' and other scripts are in '/kaggle/input'\n",
    "    kaggle_input_path = '/kaggle/usr/lib'\n",
    "    #sys.path.append(kaggle_input_path)\n",
    "    \n",
    "    #import preprocessing_py.preprocessing_py as preprocessing\n",
    "    #import models_py.models_py as models\n",
    "    #import utils_py.utils_py as utils\n",
    "   \n",
    "    \n",
    "    # Install missing libraries on kaggle\n",
    "    ! pip install torchsummary\n",
    "    ! pip install mlflow\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    import scripts.preprocessing as preprocessing\n",
    "    import scripts.models as models\n",
    "    import scripts.utils as utils\n",
    "    \n",
    "    \n",
    "    \n",
    "# Reload the module (if necessary)\n",
    "#importlib.reload(preprocessing)\n",
    "#importlib.reload(models)\n",
    "#importlib.reload(utils)\n",
    "\n",
    "# Other imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchsummary\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e45d121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.331584Z",
     "iopub.status.busy": "2024-06-14T20:19:44.331010Z",
     "iopub.status.idle": "2024-06-14T20:19:44.358377Z",
     "shell.execute_reply": "2024-06-14T20:19:44.357593Z"
    },
    "papermill": {
     "duration": 0.042439,
     "end_time": "2024-06-14T20:19:44.360588",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.318149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"/kaggle/usr/lib/models_py/models_py.py\"\n",
    "sys.path.insert(1, PATH)\n",
    "import models_py as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b88622c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.386305Z",
     "iopub.status.busy": "2024-06-14T20:19:44.385758Z",
     "iopub.status.idle": "2024-06-14T20:19:44.390662Z",
     "shell.execute_reply": "2024-06-14T20:19:44.389835Z"
    },
    "papermill": {
     "duration": 0.020079,
     "end_time": "2024-06-14T20:19:44.392661",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.372582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"/kaggle/usr/lib/utils_py/utils_py.py\"\n",
    "sys.path.insert(1, PATH)\n",
    "import utils_py as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b770e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.415964Z",
     "iopub.status.busy": "2024-06-14T20:19:44.415700Z",
     "iopub.status.idle": "2024-06-14T20:19:44.420651Z",
     "shell.execute_reply": "2024-06-14T20:19:44.419881Z"
    },
    "papermill": {
     "duration": 0.018898,
     "end_time": "2024-06-14T20:19:44.422769",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.403871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"/kaggle/usr/lib/preprocessing_py/preprocessing_py.py\"\n",
    "sys.path.insert(1, PATH)\n",
    "import preprocessing_py as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe1f489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.446012Z",
     "iopub.status.busy": "2024-06-14T20:19:44.445767Z",
     "iopub.status.idle": "2024-06-14T20:19:44.456665Z",
     "shell.execute_reply": "2024-06-14T20:19:44.455791Z"
    },
    "papermill": {
     "duration": 0.024655,
     "end_time": "2024-06-14T20:19:44.458561",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.433906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils_py' (<_frozen_importlib_external._NamespaceLoader object at 0x7b1669f3c220>)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)\n",
    "importlib.reload(models)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3853e5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.482503Z",
     "iopub.status.busy": "2024-06-14T20:19:44.482045Z",
     "iopub.status.idle": "2024-06-14T20:19:44.485690Z",
     "shell.execute_reply": "2024-06-14T20:19:44.484823Z"
    },
    "papermill": {
     "duration": 0.017006,
     "end_time": "2024-06-14T20:19:44.487527",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.470521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' #utils.use_GPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac88064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.510901Z",
     "iopub.status.busy": "2024-06-14T20:19:44.510622Z",
     "iopub.status.idle": "2024-06-14T20:19:44.526617Z",
     "shell.execute_reply": "2024-06-14T20:19:44.525795Z"
    },
    "papermill": {
     "duration": 0.029952,
     "end_time": "2024-06-14T20:19:44.528559",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.498607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FireStorm(nn.Module):\n",
    "    # This model is based on Fire module from SqueezeNet with the addition of BatchNorm \n",
    "    # and the change of ReLU to LeakyReLU\n",
    "\n",
    "    # autoencoder\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        squeeze_planes: int,\n",
    "        expand1x1_planes: int,\n",
    "        expand3x3_planes: int,\n",
    "        \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.inplanes = inplanes\n",
    "\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.LeakyReLU(inplace=True)\n",
    "        self.squeeze_bn = nn.BatchNorm2d(squeeze_planes)\n",
    "       \n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n",
    "        self.expand1x1_activation = nn.LeakyReLU(inplace=True)\n",
    "        self.expand1x1_bn = nn.BatchNorm2d(expand1x1_planes)\n",
    "        \n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.LeakyReLU(inplace=True)\n",
    "        self.expand3x3_bn = nn.BatchNorm2d(expand3x3_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_bn(self.squeeze_activation(self.squeeze(x)))\n",
    "        return torch.cat([\n",
    "            self.expand1x1_bn(\n",
    "                self.expand1x1_activation(self.expand1x1(x))\n",
    "            ),\n",
    "            self.expand3x3_bn(\n",
    "                self.expand3x3_activation(self.expand3x3(x)) \n",
    "            ),\n",
    "            ], 1)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "class StormModel2(nn.Module):\n",
    "    # version with more parameters\n",
    "\n",
    "    def __init__(self, num_classes: int = 251, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "\n",
    "            FireStorm(64, 16, 64, 64),\n",
    "            FireStorm(128, 16, 64, 64),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "\n",
    "            FireStorm(128, 32, 128, 128),\n",
    "            FireStorm(256, 32, 128, 128),\n",
    "            FireStorm(256, 48, 192, 192),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "      \n",
    "            FireStorm(384, 64, 192, 192),\n",
    "            FireStorm(384, 64, 256, 256),  # added module\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, self.num_classes),\n",
    "        )\n",
    "       \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return torch.flatten(x, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3ceb7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:24:21.972371Z",
     "start_time": "2024-06-04T14:24:21.961718Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:44.552607Z",
     "iopub.status.busy": "2024-06-14T20:19:44.552332Z",
     "iopub.status.idle": "2024-06-14T20:19:45.038814Z",
     "shell.execute_reply": "2024-06-14T20:19:45.037871Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.50128,
     "end_time": "2024-06-14T20:19:45.041251",
     "exception": false,
     "start_time": "2024-06-14T20:19:44.539971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_kaggle():\n",
    "    path_SSL = \"/kaggle/input/ssl/pytorch/uploaded/1\"\n",
    "    path_Storm = \"/kaggle/input/stormmodelpretrained/pytorch/dict-gruccia/1\"\n",
    "else:\n",
    "    path_SSL = \".\"\n",
    "    path_Storm = \".\"\n",
    "    \n",
    "\n",
    "#model = models.StormModel()\n",
    "#StormModel = model.to(device)\n",
    "\n",
    "\n",
    "#model = torch.load(os.path.join(path_Storm,\"StormModel1.0.pth\"))\n",
    "#StormModel30 = model.to(device)\n",
    "\n",
    "\n",
    "#model = models.StormModel()  # \n",
    "#model = StormModel2()\n",
    "#StormModel2 = model.to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(path_SSL,\"StormModelPretrained.pth\")))\n",
    "#model.load_state_dict(torch.load(\"/kaggle/input/stormmodelpretrained/pytorch/dict-gruccia/1/StormModelPretrained.pth\"))\n",
    "#SSLmodel = model.to(device)\n",
    "\n",
    "\n",
    "model = models.StormModel()\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/perluca/pytorch/colordifferentdatasetnonorm/1/StormModelPretrained(1).pth\"))\n",
    "#model.load_state_dict(torch.load(\"/kaggle/input/jigsawpretrain/pytorch/same_data/1/MAINjigsawpretrained_same_data.pth\"))\n",
    "#model.load_state_dict(torch.load(\"/kaggle/input/jigsawpretrain/pytorch/first/1/MAINjigsawpretrained.pth\"))\n",
    "#model=torch.load(\"/kaggle/input/final_pretrained/pytorch/samedataset_norm_12ep/1/SSLmodel_pretrained.pth\")\n",
    "SSL_StormModel = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e51d191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T14:34:04.612770Z",
     "start_time": "2024-06-04T14:34:04.593346Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:45.067825Z",
     "iopub.status.busy": "2024-06-14T20:19:45.067487Z",
     "iopub.status.idle": "2024-06-14T20:19:46.528566Z",
     "shell.execute_reply": "2024-06-14T20:19:46.527518Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.477828,
     "end_time": "2024-06-14T20:19:46.531719",
     "exception": false,
     "start_time": "2024-06-14T20:19:45.053891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 111, 111]           1,792\n",
      "         LeakyReLU-2         [-1, 64, 111, 111]               0\n",
      "         MaxPool2d-3           [-1, 64, 55, 55]               0\n",
      "            Conv2d-4           [-1, 16, 55, 55]           1,040\n",
      "         LeakyReLU-5           [-1, 16, 55, 55]               0\n",
      "       BatchNorm2d-6           [-1, 16, 55, 55]              32\n",
      "            Conv2d-7           [-1, 64, 55, 55]           1,088\n",
      "         LeakyReLU-8           [-1, 64, 55, 55]               0\n",
      "       BatchNorm2d-9           [-1, 64, 55, 55]             128\n",
      "           Conv2d-10           [-1, 64, 55, 55]           9,280\n",
      "        LeakyReLU-11           [-1, 64, 55, 55]               0\n",
      "      BatchNorm2d-12           [-1, 64, 55, 55]             128\n",
      "        FireStorm-13          [-1, 128, 55, 55]               0\n",
      "           Conv2d-14           [-1, 16, 55, 55]           2,064\n",
      "        LeakyReLU-15           [-1, 16, 55, 55]               0\n",
      "      BatchNorm2d-16           [-1, 16, 55, 55]              32\n",
      "           Conv2d-17           [-1, 64, 55, 55]           1,088\n",
      "        LeakyReLU-18           [-1, 64, 55, 55]               0\n",
      "      BatchNorm2d-19           [-1, 64, 55, 55]             128\n",
      "           Conv2d-20           [-1, 64, 55, 55]           9,280\n",
      "        LeakyReLU-21           [-1, 64, 55, 55]               0\n",
      "      BatchNorm2d-22           [-1, 64, 55, 55]             128\n",
      "        FireStorm-23          [-1, 128, 55, 55]               0\n",
      "        MaxPool2d-24          [-1, 128, 27, 27]               0\n",
      "           Conv2d-25           [-1, 32, 27, 27]           4,128\n",
      "        LeakyReLU-26           [-1, 32, 27, 27]               0\n",
      "      BatchNorm2d-27           [-1, 32, 27, 27]              64\n",
      "           Conv2d-28          [-1, 128, 27, 27]           4,224\n",
      "        LeakyReLU-29          [-1, 128, 27, 27]               0\n",
      "      BatchNorm2d-30          [-1, 128, 27, 27]             256\n",
      "           Conv2d-31          [-1, 128, 27, 27]          36,992\n",
      "        LeakyReLU-32          [-1, 128, 27, 27]               0\n",
      "      BatchNorm2d-33          [-1, 128, 27, 27]             256\n",
      "        FireStorm-34          [-1, 256, 27, 27]               0\n",
      "           Conv2d-35           [-1, 32, 27, 27]           8,224\n",
      "        LeakyReLU-36           [-1, 32, 27, 27]               0\n",
      "      BatchNorm2d-37           [-1, 32, 27, 27]              64\n",
      "           Conv2d-38          [-1, 128, 27, 27]           4,224\n",
      "        LeakyReLU-39          [-1, 128, 27, 27]               0\n",
      "      BatchNorm2d-40          [-1, 128, 27, 27]             256\n",
      "           Conv2d-41          [-1, 128, 27, 27]          36,992\n",
      "        LeakyReLU-42          [-1, 128, 27, 27]               0\n",
      "      BatchNorm2d-43          [-1, 128, 27, 27]             256\n",
      "        FireStorm-44          [-1, 256, 27, 27]               0\n",
      "        MaxPool2d-45          [-1, 256, 13, 13]               0\n",
      "           Conv2d-46           [-1, 48, 13, 13]          12,336\n",
      "        LeakyReLU-47           [-1, 48, 13, 13]               0\n",
      "      BatchNorm2d-48           [-1, 48, 13, 13]              96\n",
      "           Conv2d-49          [-1, 192, 13, 13]           9,408\n",
      "        LeakyReLU-50          [-1, 192, 13, 13]               0\n",
      "      BatchNorm2d-51          [-1, 192, 13, 13]             384\n",
      "           Conv2d-52          [-1, 192, 13, 13]          83,136\n",
      "        LeakyReLU-53          [-1, 192, 13, 13]               0\n",
      "      BatchNorm2d-54          [-1, 192, 13, 13]             384\n",
      "        FireStorm-55          [-1, 384, 13, 13]               0\n",
      "           Conv2d-56           [-1, 64, 13, 13]          24,640\n",
      "        LeakyReLU-57           [-1, 64, 13, 13]               0\n",
      "      BatchNorm2d-58           [-1, 64, 13, 13]             128\n",
      "           Conv2d-59          [-1, 256, 13, 13]          16,640\n",
      "        LeakyReLU-60          [-1, 256, 13, 13]               0\n",
      "      BatchNorm2d-61          [-1, 256, 13, 13]             512\n",
      "           Conv2d-62          [-1, 256, 13, 13]         147,712\n",
      "        LeakyReLU-63          [-1, 256, 13, 13]               0\n",
      "      BatchNorm2d-64          [-1, 256, 13, 13]             512\n",
      "        FireStorm-65          [-1, 512, 13, 13]               0\n",
      "AdaptiveAvgPool2d-66            [-1, 512, 1, 1]               0\n",
      "          Flatten-67                  [-1, 512]               0\n",
      "           Linear-68                  [-1, 512]         262,656\n",
      "        LeakyReLU-69                  [-1, 512]               0\n",
      "          Dropout-70                  [-1, 512]               0\n",
      "           Linear-71                  [-1, 251]         128,763\n",
      "================================================================\n",
      "Total params: 809,451\n",
      "Trainable params: 809,451\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 57.93\n",
      "Params size (MB): 3.09\n",
      "Estimated Total Size (MB): 61.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3, 224, 224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8bc285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T16:31:14.275625Z",
     "start_time": "2024-06-03T16:30:54.879615Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:46.557917Z",
     "iopub.status.busy": "2024-06-14T20:19:46.557582Z",
     "iopub.status.idle": "2024-06-14T20:19:46.565456Z",
     "shell.execute_reply": "2024-06-14T20:19:46.564610Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023593,
     "end_time": "2024-06-14T20:19:46.567548",
     "exception": false,
     "start_time": "2024-06-14T20:19:46.543955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "if not is_kaggle():\n",
    "\n",
    "    # check if the train folder is  already created\n",
    "    if not os.path.exists('data/train'):\n",
    "        folder_structure = preprocessing.create_dataset()\n",
    "        # transform it in a dataframe and list the number of images per class in the folders\n",
    "        a = pd.DataFrame([(k, len(v)) for k,v in folder_structure[0].items()], \n",
    "                            columns=['class', 'count'])\n",
    "        b = pd.DataFrame([(k, len(v)) for k,v in folder_structure[1].items()], \n",
    "                            columns=['class', 'count'])\n",
    "        image_counts = pd.merge(a, \n",
    "                                b, \n",
    "                                on='class', \n",
    "                                how='outer', \n",
    "                                suffixes=('_train', '_test'))\n",
    "        \n",
    "        \n",
    "        image_counts.loc[np.argmin(image_counts['count_train']),:]\n",
    "        # create a validation set\n",
    "        preprocessing.create_validation(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b18233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:46.592978Z",
     "iopub.status.busy": "2024-06-14T20:19:46.592685Z",
     "iopub.status.idle": "2024-06-14T20:19:46.596900Z",
     "shell.execute_reply": "2024-06-14T20:19:46.596078Z"
    },
    "papermill": {
     "duration": 0.019174,
     "end_time": "2024-06-14T20:19:46.598870",
     "exception": false,
     "start_time": "2024-06-14T20:19:46.579696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_kaggle():\n",
    "    im_dir = '/kaggle/input/food-dataset-sl/'\n",
    "else:\n",
    "    im_dir ='.'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1334c87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:46.624215Z",
     "iopub.status.busy": "2024-06-14T20:19:46.623921Z",
     "iopub.status.idle": "2024-06-14T20:19:46.629638Z",
     "shell.execute_reply": "2024-06-14T20:19:46.628641Z"
    },
    "papermill": {
     "duration": 0.020665,
     "end_time": "2024-06-14T20:19:46.631723",
     "exception": false,
     "start_time": "2024-06-14T20:19:46.611058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/input/food-dataset-sl/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c5060c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:08:44.782398Z",
     "start_time": "2024-06-03T17:08:44.644528Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-14T20:19:46.657466Z",
     "iopub.status.busy": "2024-06-14T20:19:46.657176Z",
     "iopub.status.idle": "2024-06-14T20:29:11.433391Z",
     "shell.execute_reply": "2024-06-14T20:29:11.432530Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 564.792098,
     "end_time": "2024-06-14T20:29:11.435921",
     "exception": false,
     "start_time": "2024-06-14T20:19:46.643823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # resize \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize pixel values\n",
    "    transforms.Normalize(mean= [0.6388, 0.5445, 0.4448],  std =  [0.2713, 0.2864, 0.3131]),\n",
    "])\n",
    "\n",
    "\n",
    "# Load the training dataset\n",
    "trainset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/train'), transform=transform)\n",
    "\n",
    "# Create data loader for training data with batch size 4 and shuffling\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "valset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/val'), transform=transform)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d459026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:29:11.460987Z",
     "iopub.status.busy": "2024-06-14T20:29:11.460671Z",
     "iopub.status.idle": "2024-06-14T20:29:11.472394Z",
     "shell.execute_reply": "2024-06-14T20:29:11.471624Z"
    },
    "papermill": {
     "duration": 0.026195,
     "end_time": "2024-06-14T20:29:11.474195",
     "exception": false,
     "start_time": "2024-06-14T20:29:11.448000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define criterion, optimizer and scheduler and other parameters for training\n",
    "opt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\n",
    "momentum = 0.9                      # momentum ONLY for SGD optimizer\n",
    "weight_decay = 1e-3                 # weight decay ONLY on Adam optimizer\n",
    "step_size = 7                       # step size for the scheduler\n",
    "gamma = 0.1                         # gamma for the scheduler\n",
    "\n",
    "batch_size = 8                      # batch size\n",
    "num_epochs=10                       # number of epochs\n",
    "patience = 3                        # patience for early stopping\n",
    "criterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\n",
    "lr = 1e-4                           # learning rate\n",
    "\n",
    "model_name = \"squeezenet\"           # model name\n",
    "model = model       # model\n",
    "\n",
    "\n",
    "\n",
    "#set the optimizer\n",
    "if opt == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "elif opt == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "else:\n",
    "    print(\"Invalid optimizer\")\n",
    "\n",
    "#set the criterion\n",
    "if criterion == \"CrossEntropyLoss\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif criterion == \"MSELoss\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif criterion == \"L1Loss\":\n",
    "    criterion = nn.L1Loss()\n",
    "elif criterion == \"NLLLoss\":\n",
    "    criterion = nn.NLLLoss()\n",
    "else:\n",
    "    print(\"Invalid criterion\")\n",
    "\n",
    "#set the scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Upload model to correct device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43d51168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:29:11.499542Z",
     "iopub.status.busy": "2024-06-14T20:29:11.499209Z",
     "iopub.status.idle": "2024-06-14T20:29:11.508691Z",
     "shell.execute_reply": "2024-06-14T20:29:11.507873Z"
    },
    "papermill": {
     "duration": 0.02505,
     "end_time": "2024-06-14T20:29:11.510712",
     "exception": false,
     "start_time": "2024-06-14T20:29:11.485662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_training_parameters(model, model_name, opt, lr, weight_decay, momentum, criterion, step_size, gamma, num_epochs, patience, device):\n",
    "\n",
    "    #set the optimizer\n",
    "    if opt == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif opt == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        print(\"Invalid optimizer\")\n",
    "\n",
    "    #set the criterion\n",
    "    if criterion == \"CrossEntropyLoss\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif criterion == \"MSELoss\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif criterion == \"L1Loss\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif criterion == \"NLLLoss\":\n",
    "        criterion = nn.NLLLoss()\n",
    "    else:\n",
    "        print(\"Invalid criterion\")\n",
    "\n",
    "    #set the scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Upload model to correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    training_parameters = {}\n",
    "    training_parameters['model'] = model\n",
    "    training_parameters['model_name'] = model_name\n",
    "    training_parameters['criterion'] = criterion\n",
    "    training_parameters['optimizer'] = optimizer\n",
    "    training_parameters['scheduler'] = scheduler\n",
    "    training_parameters['num_epochs'] = num_epochs\n",
    "    training_parameters['patience'] = patience\n",
    "    \n",
    "    return training_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a87e8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:23:52.036996Z",
     "start_time": "2024-06-03T17:20:54.527188Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-14T20:29:11.537661Z",
     "iopub.status.busy": "2024-06-14T20:29:11.537382Z",
     "iopub.status.idle": "2024-06-14T20:29:11.553747Z",
     "shell.execute_reply": "2024-06-14T20:29:11.552860Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.032832,
     "end_time": "2024-06-14T20:29:11.555697",
     "exception": false,
     "start_time": "2024-06-14T20:29:11.522865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training function based on above parameters\n",
    "def train_model(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10, patience=3 ):\n",
    "\n",
    "    mlflow.start_run(run_name=model_name)\n",
    "\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"optimizer\", opt)\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"momentum\", momentum)\n",
    "    mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "    mlflow.log_param(\"step_size\", step_size)\n",
    "    mlflow.log_param(\"gamma\", gamma)\n",
    "    mlflow.log_param(\"patience\", patience)\n",
    "\n",
    "    starting_step=0\n",
    "\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        train_loader_tqdm = tqdm.tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", \n",
    "                                unit=\"batch\")\n",
    "        ind_rloss=1\n",
    "        for inputs, labels in train_loader_tqdm:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix(loss=running_loss / ind_rloss)\n",
    "            ind_rloss +=1\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        scheduler.step()\n",
    "        mlflow.log_metric(\"train_loss\", epoch_loss, step=starting_step+epoch)\n",
    "        \n",
    "        # Validation loop (optional)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(valloader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Log validation loss and accuracy\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=starting_step+epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=starting_step+epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "                    \n",
    "    # Log the model\n",
    "    mlflow.pytorch.log_model(best_model, model_name)\n",
    "\n",
    "    # End the MLflow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2dcef",
   "metadata": {
    "papermill": {
     "duration": 0.01163,
     "end_time": "2024-06-14T20:29:11.579758",
     "exception": false,
     "start_time": "2024-06-14T20:29:11.568128",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42f4ac8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:29:11.604226Z",
     "iopub.status.busy": "2024-06-14T20:29:11.603960Z",
     "iopub.status.idle": "2024-06-14T20:29:11.611211Z",
     "shell.execute_reply": "2024-06-14T20:29:11.610354Z"
    },
    "papermill": {
     "duration": 0.021542,
     "end_time": "2024-06-14T20:29:11.613194",
     "exception": false,
     "start_time": "2024-06-14T20:29:11.591652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Save_mlruns():\n",
    "    print(\"moving to working directory...\")\n",
    "    ! cd /kaggle/working/\n",
    "    print(\"zipping directory...\")\n",
    "    ! zip -r mlruns.zip mlruns\n",
    "    print(\"!!REMEMBER TO DOWNLOAD IT FROM THE OUTPUT SECTION!!\")\n",
    "    print(\"back to home directory...\")\n",
    "    ! cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20cc9abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T20:29:11.637547Z",
     "iopub.status.busy": "2024-06-14T20:29:11.637233Z",
     "iopub.status.idle": "2024-06-14T21:24:55.518214Z",
     "shell.execute_reply": "2024-06-14T21:24:55.517000Z"
    },
    "papermill": {
     "duration": 3343.895807,
     "end_time": "2024-06-14T21:24:55.520568",
     "exception": false,
     "start_time": "2024-06-14T20:29:11.624761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 11860/11860 [05:32<00:00, 35.70batch/s, loss=5.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 5.0464\n",
      "Validation Loss: 4.6884, Validation Accuracy: 6.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 11860/11860 [04:32<00:00, 43.51batch/s, loss=4.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 4.6305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.2660, Validation Accuracy: 11.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 11860/11860 [04:37<00:00, 42.79batch/s, loss=4.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 4.4070\n",
      "Validation Loss: 4.0823, Validation Accuracy: 13.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 11860/11860 [04:44<00:00, 41.76batch/s, loss=4.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 4.2315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.9248, Validation Accuracy: 16.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 11860/11860 [04:44<00:00, 41.75batch/s, loss=4.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 4.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.7738, Validation Accuracy: 18.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 11860/11860 [04:40<00:00, 42.30batch/s, loss=3.96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 3.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.6536, Validation Accuracy: 20.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 11860/11860 [04:34<00:00, 43.18batch/s, loss=3.86]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 3.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5606, Validation Accuracy: 22.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 11860/11860 [04:33<00:00, 43.36batch/s, loss=3.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 3.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.4672, Validation Accuracy: 24.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 11860/11860 [04:55<00:00, 40.14batch/s, loss=3.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 3.6833\n",
      "Validation Loss: 3.3913, Validation Accuracy: 25.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 11860/11860 [04:40<00:00, 42.23batch/s, loss=3.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 3.6047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3486, Validation Accuracy: 26.73%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# example of training \n",
    "\n",
    "# Define criterion, optimizer and scheduler and other parameters for training\n",
    "opt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\n",
    "momentum = 0.9                      # momentum ONLY for SGD optimizer\n",
    "weight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\n",
    "step_size = 20                     # step size for the scheduler\n",
    "gamma = 0.5                         # gamma for the scheduler\n",
    "\n",
    "batch_size = 64                      # batch size\n",
    "num_epochs= 10                     # number of epochs\n",
    "patience = 5                        # patience for early stopping\n",
    "criterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\n",
    "lr = 1e-4                          # learning rate\n",
    "\n",
    "model_name = \"SSL_Storm_Model_jigsaw\"           # model name\n",
    "model =  SSL_StormModel               # model\n",
    "\n",
    "tr_param = set_training_parameters(model=model,model_name = model_name, opt=opt, lr=lr, weight_decay=weight_decay, \n",
    "                                   momentum=momentum, criterion=criterion, step_size=step_size, gamma=gamma, num_epochs=num_epochs, \n",
    "                                   patience=patience, device=device)\n",
    "\n",
    "#stop eventual mlflow runs\n",
    "mlflow.end_run()\n",
    "train_model(**tr_param, trainloader=trainloader, valloader=valloader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5175452,
     "sourceId": 8646217,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 182257654,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 182257893,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 183435265,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 53173,
     "sourceId": 63760,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 54533,
     "sourceId": 65368,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 54548,
     "sourceId": 65385,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 54642,
     "sourceId": 65498,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3981.861888,
   "end_time": "2024-06-14T21:25:09.801100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-14T20:18:47.939212",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
