{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8641640,"sourceType":"datasetVersion","datasetId":5175452},{"sourceId":182255674,"sourceType":"kernelVersion"},{"sourceId":182257654,"sourceType":"kernelVersion"},{"sourceId":182257811,"sourceType":"kernelVersion"},{"sourceId":182257893,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\nimport importlib\nimport sys\n\n# Determine the environment and import preprocessing module accordingly\ndef is_kaggle():\n    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\nif is_kaggle():\n    print(\"Running on Kaggle\")\n    # Assuming 'preprocessing.py' and other scripts are in '/kaggle/input'\n    kaggle_input_path = '/kaggle/usr/lib'\n    sys.path.append(kaggle_input_path)\n    \n    import preprocessing_py.preprocessing_py as preprocessing\n    import models_py.models_py as models\n    import utils_py.utils_py as utils\n   \n    \n    # Install missing libraries on kaggle\n    ! pip install torchsummary\n    ! pip install mlflow\nelse:\n    print(\"Running locally\")\n    import scripts.preprocessing as preprocessing\n    import scripts.models as models\n    import scripts.utils as utils\n    \n    from utils import *\n    \n# Reload the module (if necessary)\nimportlib.reload(preprocessing)\nimportlib.reload(models)\nimportlib.reload(utils)\n\n# Other imports\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nimport torchsummary\nimport torch.optim as optim\n\nimport tqdm\nimport mlflow\nimport mlflow.pytorch","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:37:08.385149Z","iopub.execute_input":"2024-06-08T20:37:08.385694Z","iopub.status.idle":"2024-06-08T20:38:02.771130Z","shell.execute_reply.started":"2024-06-08T20:37:08.385615Z","shell.execute_reply":"2024-06-08T20:38:02.769885Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Running on Kaggle\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nCollecting mlflow\n  Downloading mlflow-2.13.2-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.13.1)\nCollecting cachetools<6,>=5.0.0 (from mlflow)\n  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.41)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.11.0)\nRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.5.2)\nRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.26.4)\nRequirement already satisfied: opentelemetry-api<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\nRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.10/site-packages (from mlflow) (21.3)\nRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.2)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.20.3)\nCollecting pyarrow<16,>=4.0.0 (from mlflow)\n  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2023.3.post1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.0.1)\nCollecting querystring-parser<2 (from mlflow)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\nRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.32.3)\nRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.11.4)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.25)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4.4)\nRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.2)\nCollecting gunicorn<23 (from mlflow)\n  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.9.0)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.18)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.3)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nCollecting aniso8601<10,>=8 (from graphene<4->mlflow)\n  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.43b0)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\nDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\nDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nInstalling collected packages: aniso8601, querystring-parser, pyarrow, graphql-core, cachetools, gunicorn, graphql-relay, graphene, mlflow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 16.1.0\n    Uninstalling pyarrow-16.1.0:\n      Successfully uninstalled pyarrow-16.1.0\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 4.2.4\n    Uninstalling cachetools-4.2.4:\n      Successfully uninstalled cachetools-4.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aniso8601-9.0.1 cachetools-5.3.2 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 mlflow-2.13.2 pyarrow-15.0.2 querystring-parser-1.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"#set the correct directory for mlflow tracking\n#mlflow.set_tracking_uri(\"file:./mlruns\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:38:02.773734Z","iopub.execute_input":"2024-06-08T20:38:02.774398Z","iopub.status.idle":"2024-06-08T20:38:02.779879Z","shell.execute_reply.started":"2024-06-08T20:38:02.774363Z","shell.execute_reply":"2024-06-08T20:38:02.778526Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = utils.use_GPU()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:38:03.894997Z","iopub.execute_input":"2024-06-08T20:38:03.895417Z","iopub.status.idle":"2024-06-08T20:38:03.902746Z","shell.execute_reply.started":"2024-06-08T20:38:03.895380Z","shell.execute_reply":"2024-06-08T20:38:03.901055Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CUDA is not available.  Training on CPU ...\ncpu\n","output_type":"stream"}]},{"cell_type":"code","source":"model = models.SqueezeNet()\nmodel = model.to(device)","metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:24:21.972371Z","start_time":"2024-06-04T14:24:21.961718Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T20:38:03.906549Z","iopub.execute_input":"2024-06-08T20:38:03.907077Z","iopub.status.idle":"2024-06-08T20:38:03.985737Z","shell.execute_reply.started":"2024-06-08T20:38:03.907042Z","shell.execute_reply":"2024-06-08T20:38:03.984394Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"torchsummary.summary(model, (3, 224, 224));","metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:34:04.612770Z","start_time":"2024-06-04T14:34:04.593346Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T20:38:03.987279Z","iopub.execute_input":"2024-06-08T20:38:03.987769Z","iopub.status.idle":"2024-06-08T20:38:04.467814Z","shell.execute_reply.started":"2024-06-08T20:38:03.987725Z","shell.execute_reply":"2024-06-08T20:38:04.466460Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 111, 111]           1,792\n              ReLU-2         [-1, 64, 111, 111]               0\n         MaxPool2d-3           [-1, 64, 55, 55]               0\n            Conv2d-4           [-1, 16, 55, 55]           1,040\n              ReLU-5           [-1, 16, 55, 55]               0\n            Conv2d-6           [-1, 64, 55, 55]           1,088\n              ReLU-7           [-1, 64, 55, 55]               0\n            Conv2d-8           [-1, 64, 55, 55]           9,280\n              ReLU-9           [-1, 64, 55, 55]               0\n             Fire-10          [-1, 128, 55, 55]               0\n           Conv2d-11           [-1, 16, 55, 55]           2,064\n             ReLU-12           [-1, 16, 55, 55]               0\n           Conv2d-13           [-1, 64, 55, 55]           1,088\n             ReLU-14           [-1, 64, 55, 55]               0\n           Conv2d-15           [-1, 64, 55, 55]           9,280\n             ReLU-16           [-1, 64, 55, 55]               0\n             Fire-17          [-1, 128, 55, 55]               0\n        MaxPool2d-18          [-1, 128, 27, 27]               0\n           Conv2d-19           [-1, 32, 27, 27]           4,128\n             ReLU-20           [-1, 32, 27, 27]               0\n           Conv2d-21          [-1, 128, 27, 27]           4,224\n             ReLU-22          [-1, 128, 27, 27]               0\n           Conv2d-23          [-1, 128, 27, 27]          36,992\n             ReLU-24          [-1, 128, 27, 27]               0\n             Fire-25          [-1, 256, 27, 27]               0\n           Conv2d-26           [-1, 32, 27, 27]           8,224\n             ReLU-27           [-1, 32, 27, 27]               0\n           Conv2d-28          [-1, 128, 27, 27]           4,224\n             ReLU-29          [-1, 128, 27, 27]               0\n           Conv2d-30          [-1, 128, 27, 27]          36,992\n             ReLU-31          [-1, 128, 27, 27]               0\n             Fire-32          [-1, 256, 27, 27]               0\n        MaxPool2d-33          [-1, 256, 13, 13]               0\n           Conv2d-34           [-1, 48, 13, 13]          12,336\n             ReLU-35           [-1, 48, 13, 13]               0\n           Conv2d-36          [-1, 192, 13, 13]           9,408\n             ReLU-37          [-1, 192, 13, 13]               0\n           Conv2d-38          [-1, 192, 13, 13]          83,136\n             ReLU-39          [-1, 192, 13, 13]               0\n             Fire-40          [-1, 384, 13, 13]               0\n           Conv2d-41           [-1, 48, 13, 13]          18,480\n             ReLU-42           [-1, 48, 13, 13]               0\n           Conv2d-43          [-1, 192, 13, 13]           9,408\n             ReLU-44          [-1, 192, 13, 13]               0\n           Conv2d-45          [-1, 192, 13, 13]          83,136\n             ReLU-46          [-1, 192, 13, 13]               0\n             Fire-47          [-1, 384, 13, 13]               0\n           Conv2d-48           [-1, 64, 13, 13]          24,640\n             ReLU-49           [-1, 64, 13, 13]               0\n           Conv2d-50          [-1, 256, 13, 13]          16,640\n             ReLU-51          [-1, 256, 13, 13]               0\n           Conv2d-52          [-1, 256, 13, 13]         147,712\n             ReLU-53          [-1, 256, 13, 13]               0\n             Fire-54          [-1, 512, 13, 13]               0\n           Conv2d-55           [-1, 64, 13, 13]          32,832\n             ReLU-56           [-1, 64, 13, 13]               0\n           Conv2d-57          [-1, 256, 13, 13]          16,640\n             ReLU-58          [-1, 256, 13, 13]               0\n           Conv2d-59          [-1, 256, 13, 13]         147,712\n             ReLU-60          [-1, 256, 13, 13]               0\n             Fire-61          [-1, 512, 13, 13]               0\n          Dropout-62          [-1, 512, 13, 13]               0\n           Conv2d-63          [-1, 251, 13, 13]         128,763\n             ReLU-64          [-1, 251, 13, 13]               0\nAdaptiveAvgPool2d-65            [-1, 251, 1, 1]               0\n================================================================\nTotal params: 851,259\nTrainable params: 851,259\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 51.83\nParams size (MB): 3.25\nEstimated Total Size (MB): 55.65\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load data\nif not is_kaggle():\n\n    folder_structure = preprocessing.create_dataset()\n    # transform it in a dataframe and list the number of images per class in the folders\n    a = pd.DataFrame([(k, len(v)) for k,v in folder_structure[0].items()], \n                        columns=['class', 'count'])\n    b = pd.DataFrame([(k, len(v)) for k,v in folder_structure[1].items()], \n                        columns=['class', 'count'])\n    image_counts = pd.merge(a, \n                            b, \n                            on='class', \n                            how='outer', \n                            suffixes=('_train', '_test'))","metadata":{"ExecuteTime":{"end_time":"2024-06-03T16:31:14.275625Z","start_time":"2024-06-03T16:30:54.879615Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T20:38:04.469550Z","iopub.execute_input":"2024-06-08T20:38:04.470057Z","iopub.status.idle":"2024-06-08T20:38:04.478882Z","shell.execute_reply.started":"2024-06-08T20:38:04.470005Z","shell.execute_reply":"2024-06-08T20:38:04.477556Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if not is_kaggle():\n    image_counts.head()\n    image_counts.loc[np.argmin(image_counts['count_train']),:]\n    preprocessing.create_validation(42);","metadata":{"ExecuteTime":{"end_time":"2024-06-03T16:31:32.731631Z","start_time":"2024-06-03T16:31:32.724127Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T20:40:39.386939Z","iopub.execute_input":"2024-06-08T20:40:39.387344Z","iopub.status.idle":"2024-06-08T20:40:39.394046Z","shell.execute_reply.started":"2024-06-08T20:40:39.387314Z","shell.execute_reply":"2024-06-08T20:40:39.392729Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if is_kaggle():\n    im_dir = '/kaggle/input/food-dataset-sl/'\nelse:\n    im_dir ='.'  ","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:45:17.451920Z","iopub.execute_input":"2024-06-08T20:45:17.452368Z","iopub.status.idle":"2024-06-08T20:45:17.458274Z","shell.execute_reply.started":"2024-06-08T20:45:17.452329Z","shell.execute_reply":"2024-06-08T20:45:17.456983Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"im_dir","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:44:36.704848Z","iopub.execute_input":"2024-06-08T20:44:36.705280Z","iopub.status.idle":"2024-06-08T20:44:36.715516Z","shell.execute_reply.started":"2024-06-08T20:44:36.705248Z","shell.execute_reply":"2024-06-08T20:44:36.713887Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/food-dataset-sl'"},"metadata":{}}]},{"cell_type":"code","source":"transform = transforms.Compose([\n    # resize \n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(224),\n    transforms.ToTensor(),\n    # Normalize pixel values\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n\n# Load the training dataset\ntrainset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/train'), transform=transform)\n\n# Create data loader for training data with batch size 4 and shuffling\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\n\nvalset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/val'), transform=transform)\n\nvalloader = torch.utils.data.DataLoader(valset, batch_size=16, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n","metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:08:44.782398Z","start_time":"2024-06-03T17:08:44.644528Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T20:46:17.538573Z","iopub.execute_input":"2024-06-08T20:46:17.539740Z","iopub.status.idle":"2024-06-08T20:46:20.588965Z","shell.execute_reply.started":"2024-06-08T20:46:17.539688Z","shell.execute_reply":"2024-06-08T20:46:20.587807Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Define criterion, optimizer and scheduler and other parameters for training\nopt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\nmomentum = 0.9                      # momentum ONLY for SGD optimizer\nweight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\nstep_size = 7                       # step size for the scheduler\ngamma = 0.1                         # gamma for the scheduler\n\nbatch_size = 8                      # batch size\nnum_epochs=10                       # number of epochs\npatience = 3                        # patience for early stopping\ncriterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\nlr = 5e-5                           # learning rate\n\nmodel_name = \"squeezenet\"           # model name\nmodel = models.SqueezeNet()         # model\n\n\n\n#set the optimizer\nif opt == \"Adam\":\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\nelif opt == \"SGD\":\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\nelse:\n    print(\"Invalid optimizer\")\n\n#set the criterion\nif criterion == \"CrossEntropyLoss\":\n    criterion = nn.CrossEntropyLoss()\nelif criterion == \"MSELoss\":\n    criterion = nn.MSELoss()\nelif criterion == \"L1Loss\":\n    criterion = nn.L1Loss()\nelif criterion == \"NLLLoss\":\n    criterion = nn.NLLLoss()\nelse:\n    print(\"Invalid criterion\")\n\n#set the scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n# Upload model to correct device\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:46:44.658419Z","iopub.execute_input":"2024-06-08T20:46:44.658908Z","iopub.status.idle":"2024-06-08T20:46:44.701506Z","shell.execute_reply.started":"2024-06-08T20:46:44.658870Z","shell.execute_reply":"2024-06-08T20:46:44.700378Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def set_training_parameters(model, model_name, opt, lr, weight_decay, momentum, criterion, step_size, gamma, num_epochs, patience, device):\n\n    #set the optimizer\n    if opt == \"Adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif opt == \"SGD\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    else:\n        print(\"Invalid optimizer\")\n\n    #set the criterion\n    if criterion == \"CrossEntropyLoss\":\n        criterion = nn.CrossEntropyLoss()\n    elif criterion == \"MSELoss\":\n        criterion = nn.MSELoss()\n    elif criterion == \"L1Loss\":\n        criterion = nn.L1Loss()\n    elif criterion == \"NLLLoss\":\n        criterion = nn.NLLLoss()\n    else:\n        print(\"Invalid criterion\")\n\n    #set the scheduler\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # Upload model to correct device\n    model = model.to(device)\n    \n    training_parameters = {}\n    training_parameters['model'] = model\n    training_parameters['model_name'] = model_name\n    training_parameters['criterion'] = criterion\n    training_parameters['optimizer'] = optimizer\n    training_parameters['scheduler'] = scheduler\n    training_parameters['num_epochs'] = num_epochs\n    training_parameters['patience'] = patience\n    \n    return training_parameters","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:46:49.233337Z","iopub.execute_input":"2024-06-08T20:46:49.233815Z","iopub.status.idle":"2024-06-08T20:46:49.246606Z","shell.execute_reply.started":"2024-06-08T20:46:49.233779Z","shell.execute_reply":"2024-06-08T20:46:49.244656Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Training function based on above parameters\ndef train_model(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10, patience=3 ):\n\n    mlflow.start_run(run_name=model_name)\n\n    # Log model parameters\n    mlflow.log_param(\"optimizer\", opt)\n    mlflow.log_param(\"learning_rate\", lr)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"num_epochs\", num_epochs)\n    mlflow.log_param(\"momentum\", momentum)\n    mlflow.log_param(\"weight_decay\", weight_decay)\n    mlflow.log_param(\"step_size\", step_size)\n    mlflow.log_param(\"gamma\", gamma)\n    mlflow.log_param(\"patience\", patience)\n\n\n    patience_counter = 0\n    best_model = None\n    best_loss = np.inf\n\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        running_loss = 0.0\n        train_loader_tqdm = tqdm.tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", \n                                unit=\"batch\")\n        \n        for inputs, labels in train_loader_tqdm:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Print statistics\n            running_loss += loss.item()\n            train_loader_tqdm.set_postfix(loss=running_loss / len(trainloader))\n\n        epoch_loss = running_loss / len(trainloader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n\n        scheduler.step()\n        mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch)\n        \n        # Validation loop (optional)\n        model.eval()  # Set model to evaluation mode\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, labels in valloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        val_loss /= len(valloader)\n        val_accuracy = 100 * correct / total\n        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n        \n        # Log validation loss and accuracy\n        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n        \n        # Early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_model = model\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter > patience:\n                print(\"Early stopping\")\n                break\n                    \n    # Log the model\n    mlflow.pytorch.log_model(best_model, model_name)\n\n    # End the MLflow run\n    mlflow.end_run()\n\n    print('Finished Training')\n","metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:23:52.036996Z","start_time":"2024-06-03T17:20:54.527188Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T20:46:53.994009Z","iopub.execute_input":"2024-06-08T20:46:53.994437Z","iopub.status.idle":"2024-06-08T20:46:54.014512Z","shell.execute_reply.started":"2024-06-08T20:46:53.994398Z","shell.execute_reply":"2024-06-08T20:46:54.013183Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example of training \n\n# Define criterion, optimizer and scheduler and other parameters for training\nopt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\nmomentum = 0.9                      # momentum ONLY for SGD optimizer\nweight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\nstep_size = 7                       # step size for the scheduler\ngamma = 1                         # gamma for the scheduler\n\nbatch_size = 32                     # batch size\nnum_epochs= 30                        # number of epochs\npatience = 3                        # patience for early stopping\ncriterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\nlr = 0.001                        # learning rate\n\nmodel_name = \"squeezenet\"           # model name\nmodel = models.SqueezeNet()         # model\n\ntr_param = set_training_parameters(model=model,model_name = model_name, opt=opt, lr=lr, weight_decay=weight_decay, \n                                   momentum=momentum, criterion=criterion, step_size=step_size, gamma=gamma, num_epochs=num_epochs, \n                                   patience=patience, device=device)\n\n#stop eventual mlflow runs\nmlflow.end_run()\ntrain_model(**tr_param, trainloader=trainloader, valloader=valloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:47:06.595255Z","iopub.execute_input":"2024-06-08T20:47:06.596296Z","iopub.status.idle":"2024-06-08T20:47:45.714575Z","shell.execute_reply.started":"2024-06-08T20:47:06.596253Z","shell.execute_reply":"2024-06-08T20:47:45.712733Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Epoch 1/30:   1%|          | 106/11860 [00:38<1:10:48,  2.77batch/s, loss=0.0494]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#stop eventual mlflow runs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtr_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[32], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model_from_scratch = models.SqueezeNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nmlflow.end_run()\ntrain_model(model_from_scratch,\n            \"netFromScratch\", \n            trainloader, \n            valloader, \n            criterion, \n            optimizer, \n            scheduler,\n            num_epochs=10, \n            device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:38:05.018378Z","iopub.status.idle":"2024-06-08T20:38:05.018982Z","shell.execute_reply.started":"2024-06-08T20:38:05.018698Z","shell.execute_reply":"2024-06-08T20:38:05.018722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n#mlflow.end_run()\nmodel = models.SqueezeNet()\nmodel.load_state_dict(torch.load('netFromSSL.pth'))\ntrain_model(model,\n            \"netFromSSL\",\n            trainloader,\n            valloader,\n            criterion,\n            optimizer,\n            scheduler,\n            num_epochs=4,\n            device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:38:05.021414Z","iopub.status.idle":"2024-06-08T20:38:05.021888Z","shell.execute_reply.started":"2024-06-08T20:38:05.021662Z","shell.execute_reply":"2024-06-08T20:38:05.021692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlflow.end_run()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:38:05.023607Z","iopub.status.idle":"2024-06-08T20:38:05.024085Z","shell.execute_reply.started":"2024-06-08T20:38:05.023878Z","shell.execute_reply":"2024-06-08T20:38:05.023897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#best hyperparameters from github non provato\n\n# transformations\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n}\n\ndata_dir = 'path_to_foodx251_dataset'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the model\nmodel = models.squeezenet1_1(pretrained=True)\nmodel.classifier[1] = nn.Conv2d(512, len(class_names), kernel_size=(1,1), stride=(1,1))\nmodel.num_classes = len(class_names)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# Training function\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        for inputs, labels in dataloaders['train']:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        \n        epoch_loss = running_loss / dataset_sizes['train']\n        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n        scheduler.step()\n\n        # Validation loop\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, labels in dataloaders['val']:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        val_loss /= dataset_sizes['val']\n        val_accuracy = correct / total\n        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n\n    return model\n\n# Train the model\nmodel = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=25)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:38:05.026449Z","iopub.status.idle":"2024-06-08T20:38:05.026942Z","shell.execute_reply.started":"2024-06-08T20:38:05.026717Z","shell.execute_reply":"2024-06-08T20:38:05.026739Z"},"trusted":true},"execution_count":null,"outputs":[]}]}