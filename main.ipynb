{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8641640,"sourceType":"datasetVersion","datasetId":5175452},{"sourceId":182255674,"sourceType":"kernelVersion"},{"sourceId":182257654,"sourceType":"kernelVersion"},{"sourceId":182257811,"sourceType":"kernelVersion"},{"sourceId":182257893,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\nimport importlib\nimport sys\n\n# Determine the environment and import preprocessing module accordingly\ndef is_kaggle():\n    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\nif is_kaggle():\n    print(\"Running on Kaggle\")\n    # Assuming 'preprocessing.py' and other scripts are in '/kaggle/input'\n    kaggle_input_path = '/kaggle/usr/lib'\n    sys.path.append(kaggle_input_path)\n    \n    import preprocessing_py.preprocessing_py as preprocessing\n    import models_py.models_py as models\n    import utils_py.utils_py as utils\n   \n    \n    # Install missing libraries on kaggle\n    ! pip install torchsummary\n    ! pip install mlflow\nelse:\n    print(\"Running locally\")\n    import scripts.preprocessing as preprocessing\n    import scripts.models as models\n    import scripts.utils as utils\n    \n    from utils import *\n    \n# Reload the module (if necessary)\nimportlib.reload(preprocessing)\nimportlib.reload(models)\nimportlib.reload(utils)\n\n# Other imports\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nimport torchsummary\nimport torch.optim as optim\n\nimport tqdm\nimport mlflow\nimport mlflow.pytorch","metadata":{"execution":{"iopub.status.busy":"2024-06-09T08:52:41.109503Z","iopub.execute_input":"2024-06-09T08:52:41.109849Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Running on Kaggle\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","output_type":"stream"}]},{"cell_type":"code","source":"#set the correct directory for mlflow tracking\n#mlflow.set_tracking_uri(\"file:./mlruns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = utils.use_GPU()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.SqueezeNet()\nmodel = model.to(device)","metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:24:21.972371Z","start_time":"2024-06-04T14:24:21.961718Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torchsummary.summary(model, (3, 224, 224));","metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:34:04.612770Z","start_time":"2024-06-04T14:34:04.593346Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\nif not is_kaggle():\n\n    folder_structure = preprocessing.create_dataset()\n    # transform it in a dataframe and list the number of images per class in the folders\n    a = pd.DataFrame([(k, len(v)) for k,v in folder_structure[0].items()], \n                        columns=['class', 'count'])\n    b = pd.DataFrame([(k, len(v)) for k,v in folder_structure[1].items()], \n                        columns=['class', 'count'])\n    image_counts = pd.merge(a, \n                            b, \n                            on='class', \n                            how='outer', \n                            suffixes=('_train', '_test'))","metadata":{"ExecuteTime":{"end_time":"2024-06-03T16:31:14.275625Z","start_time":"2024-06-03T16:30:54.879615Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not is_kaggle():\n    image_counts.head()\n    image_counts.loc[np.argmin(image_counts['count_train']),:]\n    preprocessing.create_validation(42);","metadata":{"ExecuteTime":{"end_time":"2024-06-03T16:31:32.731631Z","start_time":"2024-06-03T16:31:32.724127Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_kaggle():\n    im_dir = '/kaggle/input/food-dataset-sl/'\nelse:\n    im_dir ='.'  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    # resize \n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(224),\n    transforms.ToTensor(),\n    # Normalize pixel values\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n\n# Load the training dataset\ntrainset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/train'), transform=transform)\n\n# Create data loader for training data with batch size 4 and shuffling\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\n\nvalset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/val'), transform=transform)\n\nvalloader = torch.utils.data.DataLoader(valset, batch_size=16, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n","metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:08:44.782398Z","start_time":"2024-06-03T17:08:44.644528Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define criterion, optimizer and scheduler and other parameters for training\nopt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\nmomentum = 0.9                      # momentum ONLY for SGD optimizer\nweight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\nstep_size = 7                       # step size for the scheduler\ngamma = 0.1                         # gamma for the scheduler\n\nbatch_size = 8                      # batch size\nnum_epochs=10                       # number of epochs\npatience = 3                        # patience for early stopping\ncriterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\nlr = 5e-5                           # learning rate\n\nmodel_name = \"squeezenet\"           # model name\nmodel = models.SqueezeNet()         # model\n\n\n\n#set the optimizer\nif opt == \"Adam\":\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\nelif opt == \"SGD\":\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\nelse:\n    print(\"Invalid optimizer\")\n\n#set the criterion\nif criterion == \"CrossEntropyLoss\":\n    criterion = nn.CrossEntropyLoss()\nelif criterion == \"MSELoss\":\n    criterion = nn.MSELoss()\nelif criterion == \"L1Loss\":\n    criterion = nn.L1Loss()\nelif criterion == \"NLLLoss\":\n    criterion = nn.NLLLoss()\nelse:\n    print(\"Invalid criterion\")\n\n#set the scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n# Upload model to correct device\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_training_parameters(model, model_name, opt, lr, weight_decay, momentum, criterion, step_size, gamma, num_epochs, patience, device):\n\n    #set the optimizer\n    if opt == \"Adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif opt == \"SGD\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    else:\n        print(\"Invalid optimizer\")\n\n    #set the criterion\n    if criterion == \"CrossEntropyLoss\":\n        criterion = nn.CrossEntropyLoss()\n    elif criterion == \"MSELoss\":\n        criterion = nn.MSELoss()\n    elif criterion == \"L1Loss\":\n        criterion = nn.L1Loss()\n    elif criterion == \"NLLLoss\":\n        criterion = nn.NLLLoss()\n    else:\n        print(\"Invalid criterion\")\n\n    #set the scheduler\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # Upload model to correct device\n    model = model.to(device)\n    \n    training_parameters = {}\n    training_parameters['model'] = model\n    training_parameters['model_name'] = model_name\n    training_parameters['criterion'] = criterion\n    training_parameters['optimizer'] = optimizer\n    training_parameters['scheduler'] = scheduler\n    training_parameters['num_epochs'] = num_epochs\n    training_parameters['patience'] = patience\n    \n    return training_parameters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training function based on above parameters\ndef train_model(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10, patience=3 ):\n\n    mlflow.start_run(run_name=model_name)\n\n    # Log model parameters\n    mlflow.log_param(\"optimizer\", opt)\n    mlflow.log_param(\"learning_rate\", lr)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"num_epochs\", num_epochs)\n    mlflow.log_param(\"momentum\", momentum)\n    mlflow.log_param(\"weight_decay\", weight_decay)\n    mlflow.log_param(\"step_size\", step_size)\n    mlflow.log_param(\"gamma\", gamma)\n    mlflow.log_param(\"patience\", patience)\n\n\n    patience_counter = 0\n    best_model = None\n    best_loss = np.inf\n        \n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        running_loss = 0.0\n        train_loader_tqdm = tqdm.tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", \n                                unit=\"batch\")\n        ind_rloss=1\n        for inputs, labels in train_loader_tqdm:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Print statistics\n            running_loss += loss.item()\n            train_loader_tqdm.set_postfix(loss=running_loss / ind_rloss)\n            ind_rloss +=1\n\n        epoch_loss = running_loss / len(trainloader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n\n        scheduler.step()\n        mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch)\n        \n        # Validation loop (optional)\n        model.eval()  # Set model to evaluation mode\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, labels in valloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        val_loss /= len(valloader)\n        val_accuracy = 100 * correct / total\n        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n        \n        # Log validation loss and accuracy\n        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n        \n        # Early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_model = model\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter > patience:\n                print(\"Early stopping\")\n                break\n                    \n    # Log the model\n    mlflow.pytorch.log_model(best_model, model_name)\n\n    # End the MLflow run\n    mlflow.end_run()\n\n    print('Finished Training')\n","metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:23:52.036996Z","start_time":"2024-06-03T17:20:54.527188Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def Save_mlruns():\n    print(\"moving to working directory...\")\n    ! cd /kaggle/working/\n    print(\"zipping directory...\")\n    ! zip -r mlruns.zip mlruns\n    print(\"!!REMEMBER TO DOWNLOAD IT FROM THE OUTPUT SECTION!!\")\n    print(\"back to home directory...\")\n    ! cd ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example of training \n\n# Define criterion, optimizer and scheduler and other parameters for training\nopt = \"SGD\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\nmomentum = 0.9                      # momentum ONLY for SGD optimizer\nweight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\nstep_size = 5                       # step size for the scheduler\ngamma = 0.1                         # gamma for the scheduler\n\nbatch_size = 64                      # batch size\nnum_epochs= 30                        # number of epochs\npatience = 5                        # patience for early stopping\ncriterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\nlr = 0.01                        # learning rate\n\nmodel_name = \"squeezenet_SGD\"           # model name\nmodel = models.SqueezeNet()         # model\n\ntr_param = set_training_parameters(model=model,model_name = model_name, opt=opt, lr=lr, weight_decay=weight_decay, \n                                   momentum=momentum, criterion=criterion, step_size=step_size, gamma=gamma, num_epochs=num_epochs, \n                                   patience=patience, device=device)\n\n#stop eventual mlflow runs\nmlflow.end_run()\ntrain_model(**tr_param, trainloader=trainloader, valloader=valloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Save_mlruns()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}