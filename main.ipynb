{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8646217,"sourceType":"datasetVersion","datasetId":5175452},{"sourceId":182255674,"sourceType":"kernelVersion"},{"sourceId":182257654,"sourceType":"kernelVersion"},{"sourceId":182257811,"sourceType":"kernelVersion"},{"sourceId":182257893,"sourceType":"kernelVersion"},{"sourceId":63760,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":53173}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\nimport importlib\nimport sys\n\n# Determine the environment and import preprocessing module accordingly\ndef is_kaggle():\n    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\nif is_kaggle():\n    print(\"Running on Kaggle\")\n    # Assuming 'preprocessing.py' and other scripts are in '/kaggle/input'\n    kaggle_input_path = '/kaggle/usr/lib'\n    sys.path.append(kaggle_input_path)\n    \n    import preprocessing_py.preprocessing_py as preprocessing\n    import models_py.models_py as models\n    import utils_py.utils_py as utils\n   \n    \n    # Install missing libraries on kaggle\n    ! pip install torchsummary\n    ! pip install mlflow\nelse:\n    print(\"Running locally\")\n    import scripts.preprocessing as preprocessing\n    import scripts.models as models\n    import scripts.utils as utils\n    \n    \n    \n# Reload the module (if necessary)\nimportlib.reload(preprocessing)\nimportlib.reload(models)\nimportlib.reload(utils)\n\n# Other imports\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nimport torchsummary\nimport torch.optim as optim\n\nimport tqdm\nimport mlflow\nimport mlflow.pytorch","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:58:33.996688Z","iopub.execute_input":"2024-06-10T19:58:33.997605Z","iopub.status.idle":"2024-06-10T19:58:58.957027Z","shell.execute_reply.started":"2024-06-10T19:58:33.997558Z","shell.execute_reply":"2024-06-10T19:58:58.955926Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Running on Kaggle\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\nRequirement already satisfied: mlflow in /opt/conda/lib/python3.10/site-packages (2.13.2)\nRequirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.13.1)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (5.3.2)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.41)\nRequirement already satisfied: graphene<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.3)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.11.0)\nRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.5.2)\nRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.26.4)\nRequirement already satisfied: opentelemetry-api<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\nRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.10/site-packages (from mlflow) (21.3)\nRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.20.3)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (14.0.2)\nRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2023.3.post1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.0.1)\nRequirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.32.3)\nRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.11.4)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.25)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4.4)\nRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.2)\nRequirement already satisfied: gunicorn<23 in /opt/conda/lib/python3.10/site-packages (from mlflow) (22.0.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.9.0)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.18)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.3)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.3)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: aniso8601<10,>=8 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow) (9.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.43b0)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = utils.use_GPU()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:58:58.959605Z","iopub.execute_input":"2024-06-10T19:58:58.960468Z","iopub.status.idle":"2024-06-10T19:58:58.965787Z","shell.execute_reply.started":"2024-06-10T19:58:58.960423Z","shell.execute_reply":"2024-06-10T19:58:58.964936Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CUDA is available!  Training on GPU ...\ncuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"if is_kaggle():\n    path = \"/kaggle/input/ssl/pytorch/uploaded/1\"\nelse:\n    path=\".\"\n    \n\nmodel = models.StormSqueezeNet()\nStormModel = model.to(device)\n\nmodel = models.SqueezeNet()\nmodel.load_state_dict(torch.load(os.path.join(path,\"netFromSSL_10e.pth\")))\nSSLmodel = model.to(device)","metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:24:21.972371Z","start_time":"2024-06-04T14:24:21.961718Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-10T19:58:58.966964Z","iopub.execute_input":"2024-06-10T19:58:58.967295Z","iopub.status.idle":"2024-06-10T19:58:59.047559Z","shell.execute_reply.started":"2024-06-10T19:58:58.967265Z","shell.execute_reply":"2024-06-10T19:58:59.046796Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"torchsummary.summary(model, (3, 224, 224));","metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:34:04.612770Z","start_time":"2024-06-04T14:34:04.593346Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-10T19:58:59.048865Z","iopub.execute_input":"2024-06-10T19:58:59.049120Z","iopub.status.idle":"2024-06-10T19:59:00.549592Z","shell.execute_reply.started":"2024-06-10T19:58:59.049098Z","shell.execute_reply":"2024-06-10T19:59:00.548660Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 111, 111]           1,792\n              ReLU-2         [-1, 64, 111, 111]               0\n         MaxPool2d-3           [-1, 64, 55, 55]               0\n            Conv2d-4           [-1, 16, 55, 55]           1,040\n              ReLU-5           [-1, 16, 55, 55]               0\n            Conv2d-6           [-1, 64, 55, 55]           1,088\n              ReLU-7           [-1, 64, 55, 55]               0\n            Conv2d-8           [-1, 64, 55, 55]           9,280\n              ReLU-9           [-1, 64, 55, 55]               0\n             Fire-10          [-1, 128, 55, 55]               0\n           Conv2d-11           [-1, 16, 55, 55]           2,064\n             ReLU-12           [-1, 16, 55, 55]               0\n           Conv2d-13           [-1, 64, 55, 55]           1,088\n             ReLU-14           [-1, 64, 55, 55]               0\n           Conv2d-15           [-1, 64, 55, 55]           9,280\n             ReLU-16           [-1, 64, 55, 55]               0\n             Fire-17          [-1, 128, 55, 55]               0\n        MaxPool2d-18          [-1, 128, 27, 27]               0\n           Conv2d-19           [-1, 32, 27, 27]           4,128\n             ReLU-20           [-1, 32, 27, 27]               0\n           Conv2d-21          [-1, 128, 27, 27]           4,224\n             ReLU-22          [-1, 128, 27, 27]               0\n           Conv2d-23          [-1, 128, 27, 27]          36,992\n             ReLU-24          [-1, 128, 27, 27]               0\n             Fire-25          [-1, 256, 27, 27]               0\n           Conv2d-26           [-1, 32, 27, 27]           8,224\n             ReLU-27           [-1, 32, 27, 27]               0\n           Conv2d-28          [-1, 128, 27, 27]           4,224\n             ReLU-29          [-1, 128, 27, 27]               0\n           Conv2d-30          [-1, 128, 27, 27]          36,992\n             ReLU-31          [-1, 128, 27, 27]               0\n             Fire-32          [-1, 256, 27, 27]               0\n        MaxPool2d-33          [-1, 256, 13, 13]               0\n           Conv2d-34           [-1, 48, 13, 13]          12,336\n             ReLU-35           [-1, 48, 13, 13]               0\n           Conv2d-36          [-1, 192, 13, 13]           9,408\n             ReLU-37          [-1, 192, 13, 13]               0\n           Conv2d-38          [-1, 192, 13, 13]          83,136\n             ReLU-39          [-1, 192, 13, 13]               0\n             Fire-40          [-1, 384, 13, 13]               0\n           Conv2d-41           [-1, 48, 13, 13]          18,480\n             ReLU-42           [-1, 48, 13, 13]               0\n           Conv2d-43          [-1, 192, 13, 13]           9,408\n             ReLU-44          [-1, 192, 13, 13]               0\n           Conv2d-45          [-1, 192, 13, 13]          83,136\n             ReLU-46          [-1, 192, 13, 13]               0\n             Fire-47          [-1, 384, 13, 13]               0\n           Conv2d-48           [-1, 64, 13, 13]          24,640\n             ReLU-49           [-1, 64, 13, 13]               0\n           Conv2d-50          [-1, 256, 13, 13]          16,640\n             ReLU-51          [-1, 256, 13, 13]               0\n           Conv2d-52          [-1, 256, 13, 13]         147,712\n             ReLU-53          [-1, 256, 13, 13]               0\n             Fire-54          [-1, 512, 13, 13]               0\n           Conv2d-55           [-1, 64, 13, 13]          32,832\n             ReLU-56           [-1, 64, 13, 13]               0\n           Conv2d-57          [-1, 256, 13, 13]          16,640\n             ReLU-58          [-1, 256, 13, 13]               0\n           Conv2d-59          [-1, 256, 13, 13]         147,712\n             ReLU-60          [-1, 256, 13, 13]               0\n             Fire-61          [-1, 512, 13, 13]               0\n          Dropout-62          [-1, 512, 13, 13]               0\n           Conv2d-63          [-1, 251, 13, 13]         128,763\n             ReLU-64          [-1, 251, 13, 13]               0\nAdaptiveAvgPool2d-65            [-1, 251, 1, 1]               0\n================================================================\nTotal params: 851,259\nTrainable params: 851,259\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 51.83\nParams size (MB): 3.25\nEstimated Total Size (MB): 55.65\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load data\nif not is_kaggle():\n\n    # check if the train folder is  already created\n    if not os.path.exists('data/train'):\n        folder_structure = preprocessing.create_dataset()\n        # transform it in a dataframe and list the number of images per class in the folders\n        a = pd.DataFrame([(k, len(v)) for k,v in folder_structure[0].items()], \n                            columns=['class', 'count'])\n        b = pd.DataFrame([(k, len(v)) for k,v in folder_structure[1].items()], \n                            columns=['class', 'count'])\n        image_counts = pd.merge(a, \n                                b, \n                                on='class', \n                                how='outer', \n                                suffixes=('_train', '_test'))\n        \n        \n        image_counts.loc[np.argmin(image_counts['count_train']),:]\n        # create a validation set\n        preprocessing.create_validation(42);","metadata":{"ExecuteTime":{"end_time":"2024-06-03T16:31:14.275625Z","start_time":"2024-06-03T16:30:54.879615Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-10T19:59:00.552905Z","iopub.execute_input":"2024-06-10T19:59:00.553363Z","iopub.status.idle":"2024-06-10T19:59:00.560979Z","shell.execute_reply.started":"2024-06-10T19:59:00.553326Z","shell.execute_reply":"2024-06-10T19:59:00.559974Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:00.562248Z","iopub.execute_input":"2024-06-10T19:59:00.562568Z","iopub.status.idle":"2024-06-10T19:59:01.539966Z","shell.execute_reply.started":"2024-06-10T19:59:00.562537Z","shell.execute_reply":"2024-06-10T19:59:01.538694Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if is_kaggle():\n    im_dir = '/kaggle/input/food-dataset-sl/'\nelse:\n    im_dir ='.'  ","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:01.541706Z","iopub.execute_input":"2024-06-10T19:59:01.542045Z","iopub.status.idle":"2024-06-10T19:59:01.547434Z","shell.execute_reply.started":"2024-06-10T19:59:01.542014Z","shell.execute_reply":"2024-06-10T19:59:01.546464Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"im_dir","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:01.548789Z","iopub.execute_input":"2024-06-10T19:59:01.549064Z","iopub.status.idle":"2024-06-10T19:59:01.564268Z","shell.execute_reply.started":"2024-06-10T19:59:01.549041Z","shell.execute_reply":"2024-06-10T19:59:01.563388Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/food-dataset-sl/'"},"metadata":{}}]},{"cell_type":"code","source":"transform = transforms.Compose([\n    # resize \n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(224),\n    transforms.ToTensor(),\n    # Normalize pixel values\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n\n# Load the training dataset\ntrainset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/train'), transform=transform)\n\n# Create data loader for training data with batch size 4 and shuffling\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=4)\n\nvalset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/val'), transform=transform)\n\nvalloader = torch.utils.data.DataLoader(valset, batch_size=16, shuffle=True, num_workers=4)\n\ntestset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=4)\n","metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:08:44.782398Z","start_time":"2024-06-03T17:08:44.644528Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-10T19:59:01.565500Z","iopub.execute_input":"2024-06-10T19:59:01.565819Z","iopub.status.idle":"2024-06-10T19:59:36.809058Z","shell.execute_reply.started":"2024-06-10T19:59:01.565789Z","shell.execute_reply":"2024-06-10T19:59:36.808045Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define criterion, optimizer and scheduler and other parameters for training\nopt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\nmomentum = 0.9                      # momentum ONLY for SGD optimizer\nweight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\nstep_size = 7                       # step size for the scheduler\ngamma = 0.1                         # gamma for the scheduler\n\nbatch_size = 8                      # batch size\nnum_epochs=10                       # number of epochs\npatience = 3                        # patience for early stopping\ncriterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\nlr = 5e-5                           # learning rate\n\nmodel_name = \"squeezenet\"           # model name\nmodel = models.SqueezeNet()         # model\n\n\n\n#set the optimizer\nif opt == \"Adam\":\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\nelif opt == \"SGD\":\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\nelse:\n    print(\"Invalid optimizer\")\n\n#set the criterion\nif criterion == \"CrossEntropyLoss\":\n    criterion = nn.CrossEntropyLoss()\nelif criterion == \"MSELoss\":\n    criterion = nn.MSELoss()\nelif criterion == \"L1Loss\":\n    criterion = nn.L1Loss()\nelif criterion == \"NLLLoss\":\n    criterion = nn.NLLLoss()\nelse:\n    print(\"Invalid criterion\")\n\n#set the scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n# Upload model to correct device\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:36.810607Z","iopub.execute_input":"2024-06-10T19:59:36.810913Z","iopub.status.idle":"2024-06-10T19:59:36.848189Z","shell.execute_reply.started":"2024-06-10T19:59:36.810887Z","shell.execute_reply":"2024-06-10T19:59:36.847489Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def set_training_parameters(model, model_name, opt, lr, weight_decay, momentum, criterion, step_size, gamma, num_epochs, patience, device):\n\n    #set the optimizer\n    if opt == \"Adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif opt == \"SGD\":\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    else:\n        print(\"Invalid optimizer\")\n\n    #set the criterion\n    if criterion == \"CrossEntropyLoss\":\n        criterion = nn.CrossEntropyLoss()\n    elif criterion == \"MSELoss\":\n        criterion = nn.MSELoss()\n    elif criterion == \"L1Loss\":\n        criterion = nn.L1Loss()\n    elif criterion == \"NLLLoss\":\n        criterion = nn.NLLLoss()\n    else:\n        print(\"Invalid criterion\")\n\n    #set the scheduler\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    # Upload model to correct device\n    model = model.to(device)\n    \n    training_parameters = {}\n    training_parameters['model'] = model\n    training_parameters['model_name'] = model_name\n    training_parameters['criterion'] = criterion\n    training_parameters['optimizer'] = optimizer\n    training_parameters['scheduler'] = scheduler\n    training_parameters['num_epochs'] = num_epochs\n    training_parameters['patience'] = patience\n    \n    return training_parameters","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:36.849177Z","iopub.execute_input":"2024-06-10T19:59:36.849444Z","iopub.status.idle":"2024-06-10T19:59:36.858183Z","shell.execute_reply.started":"2024-06-10T19:59:36.849420Z","shell.execute_reply":"2024-06-10T19:59:36.857323Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Training function based on above parameters\ndef train_model(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10, patience=3 ):\n\n    mlflow.start_run(run_name=model_name)\n\n    # Log model parameters\n    mlflow.log_param(\"optimizer\", opt)\n    mlflow.log_param(\"learning_rate\", lr)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"num_epochs\", num_epochs)\n    mlflow.log_param(\"momentum\", momentum)\n    mlflow.log_param(\"weight_decay\", weight_decay)\n    mlflow.log_param(\"step_size\", step_size)\n    mlflow.log_param(\"gamma\", gamma)\n    mlflow.log_param(\"patience\", patience)\n\n\n    patience_counter = 0\n    best_model = None\n    best_loss = np.inf\n        \n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        running_loss = 0.0\n        train_loader_tqdm = tqdm.tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", \n                                unit=\"batch\")\n        ind_rloss=1\n        for inputs, labels in train_loader_tqdm:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            # Print statistics\n            running_loss += loss.item()\n            train_loader_tqdm.set_postfix(loss=running_loss / ind_rloss)\n            ind_rloss +=1\n\n        epoch_loss = running_loss / len(trainloader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n\n        scheduler.step()\n        mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch)\n        \n        # Validation loop (optional)\n        model.eval()  # Set model to evaluation mode\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, labels in valloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        val_loss /= len(valloader)\n        val_accuracy = 100 * correct / total\n        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n        \n        # Log validation loss and accuracy\n        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n        \n        # Early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_model = model\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter > patience:\n                print(\"Early stopping\")\n                break\n                    \n    # Log the model\n    mlflow.pytorch.log_model(best_model, model_name)\n\n    # End the MLflow run\n    mlflow.end_run()\n\n    print('Finished Training')\n","metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:23:52.036996Z","start_time":"2024-06-03T17:20:54.527188Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-10T19:59:36.859404Z","iopub.execute_input":"2024-06-10T19:59:36.859655Z","iopub.status.idle":"2024-06-10T19:59:36.877774Z","shell.execute_reply.started":"2024-06-10T19:59:36.859633Z","shell.execute_reply":"2024-06-10T19:59:36.877074Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def Save_mlruns():\n    print(\"moving to working directory...\")\n    ! cd /kaggle/working/\n    print(\"zipping directory...\")\n    ! zip -r mlruns.zip mlruns\n    print(\"!!REMEMBER TO DOWNLOAD IT FROM THE OUTPUT SECTION!!\")\n    print(\"back to home directory...\")\n    ! cd ","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:36.878797Z","iopub.execute_input":"2024-06-10T19:59:36.879067Z","iopub.status.idle":"2024-06-10T19:59:36.895306Z","shell.execute_reply.started":"2024-06-10T19:59:36.879042Z","shell.execute_reply":"2024-06-10T19:59:36.894497Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# example of training \n\n# Define criterion, optimizer and scheduler and other parameters for training\nopt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\nmomentum = 0.9                      # momentum ONLY for SGD optimizer\nweight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\nstep_size = 50                      # step size for the scheduler\ngamma = 0.5                         # gamma for the scheduler\n\nbatch_size = 64                      # batch size\nnum_epochs=10                       # number of epochs\npatience = 3                        # patience for early stopping\ncriterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\nlr = 5e-5                           # learning rate\n\nmodel_name = \"test-remote-kaggle\"           # model name\nmodel =  SSLmodel      # model, StormModel, SSLmodel\n\ntr_param = set_training_parameters(model=model,model_name = model_name, opt=opt, lr=lr, weight_decay=weight_decay, \n                                   momentum=momentum, criterion=criterion, step_size=step_size, gamma=gamma, num_epochs=num_epochs, \n                                   patience=patience, device=device)\n\n#stop eventual mlflow runs\nmlflow.end_run()\ntrain_model(**tr_param, trainloader=trainloader, valloader=valloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:59:36.899010Z","iopub.execute_input":"2024-06-10T19:59:36.899829Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/10:  19%|█▉        | 2281/11860 [00:58<05:22, 29.68batch/s, loss=5.51]","output_type":"stream"}]},{"cell_type":"code","source":"if is_kaggle():\n    Save_mlruns()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}