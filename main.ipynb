{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-13T16:36:11.004808Z","iopub.status.busy":"2024-06-13T16:36:11.004285Z"},"papermill":{"duration":41.038794,"end_time":"2024-06-11T06:35:57.624259","exception":false,"start_time":"2024-06-11T06:35:16.585465","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running locally\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import shutil\n","import importlib\n","import sys\n","\n","# Determine the environment and import preprocessing module accordingly\n","def is_kaggle():\n","    return 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n","\n","if is_kaggle():\n","    print(\"Running on Kaggle\")\n","    # Assuming 'preprocessing.py' and other scripts are in '/kaggle/input'\n","    kaggle_input_path = '/kaggle/usr/lib'\n","    sys.path.append(kaggle_input_path)\n","    \n","    import preprocessing_py.preprocessing_py as preprocessing\n","    import models_py.models_py as models\n","    import utils_py.utils_py as utils\n","   \n","    \n","    # Install missing libraries on kaggle\n","    ! pip install torchsummary\n","    ! pip install mlflow\n","else:\n","    print(\"Running locally\")\n","    import scripts.preprocessing as preprocessing\n","    import scripts.models as models\n","    import scripts.utils as utils\n","    \n","    \n","    \n","# Reload the module (if necessary)\n","importlib.reload(preprocessing)\n","importlib.reload(models)\n","importlib.reload(utils)\n","\n","# Other imports\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms\n","import torchsummary\n","import torch.optim as optim\n","\n","import tqdm\n","import mlflow\n","import mlflow.pytorch"]},{"cell_type":"code","execution_count":2,"metadata":{"papermill":{"duration":0.040754,"end_time":"2024-06-11T06:35:57.674436","exception":false,"start_time":"2024-06-11T06:35:57.633682","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available!  Training on GPU ...\n","cuda:0\n"]}],"source":["device = utils.use_GPU()"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:24:21.972371Z","start_time":"2024-06-04T14:24:21.961718Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.658015,"end_time":"2024-06-11T06:35:59.369751","exception":false,"start_time":"2024-06-11T06:35:58.711736","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["if is_kaggle():\n","    path_SSL = \"/kaggle/input/...\"   # insert paths to the model in kaggle\n","    path_Storm = \"/kaggle/input/... \" # insert paths to the model in kaggle\n","else:\n","    path_SSL = \".\"   # insert paths to the model for local runs\n","    path_Storm = \".\" # insert paths to the model for local runs\n","    \n","\n","\n","model = models.StormModel()    # first version of the model\n","#model = models.StormModel2()  # second version of the model\n","\n","# use this string to load a state dict from a file\n","# model=torch.load(\"path_Storm\")\n","\n","# use this string to load a dict from a file\n","# model.load_state_dict(torch.load(\"path_Storm\")))\n","\n","\n","StormModel = model.to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"ExecuteTime":{"end_time":"2024-06-04T14:34:04.612770Z","start_time":"2024-06-04T14:34:04.593346Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.868158,"end_time":"2024-06-11T06:36:00.249270","exception":false,"start_time":"2024-06-11T06:35:59.381112","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 111, 111]           1,792\n","         LeakyReLU-2         [-1, 64, 111, 111]               0\n","         MaxPool2d-3           [-1, 64, 55, 55]               0\n","            Conv2d-4           [-1, 16, 55, 55]           1,040\n","         LeakyReLU-5           [-1, 16, 55, 55]               0\n","       BatchNorm2d-6           [-1, 16, 55, 55]              32\n","            Conv2d-7           [-1, 64, 55, 55]           1,088\n","         LeakyReLU-8           [-1, 64, 55, 55]               0\n","       BatchNorm2d-9           [-1, 64, 55, 55]             128\n","           Conv2d-10           [-1, 64, 55, 55]           9,280\n","        LeakyReLU-11           [-1, 64, 55, 55]               0\n","      BatchNorm2d-12           [-1, 64, 55, 55]             128\n","        FireStorm-13          [-1, 128, 55, 55]               0\n","           Conv2d-14           [-1, 16, 55, 55]           2,064\n","        LeakyReLU-15           [-1, 16, 55, 55]               0\n","      BatchNorm2d-16           [-1, 16, 55, 55]              32\n","           Conv2d-17           [-1, 64, 55, 55]           1,088\n","        LeakyReLU-18           [-1, 64, 55, 55]               0\n","      BatchNorm2d-19           [-1, 64, 55, 55]             128\n","           Conv2d-20           [-1, 64, 55, 55]           9,280\n","        LeakyReLU-21           [-1, 64, 55, 55]               0\n","      BatchNorm2d-22           [-1, 64, 55, 55]             128\n","        FireStorm-23          [-1, 128, 55, 55]               0\n","        MaxPool2d-24          [-1, 128, 27, 27]               0\n","           Conv2d-25           [-1, 32, 27, 27]           4,128\n","        LeakyReLU-26           [-1, 32, 27, 27]               0\n","      BatchNorm2d-27           [-1, 32, 27, 27]              64\n","           Conv2d-28          [-1, 128, 27, 27]           4,224\n","        LeakyReLU-29          [-1, 128, 27, 27]               0\n","      BatchNorm2d-30          [-1, 128, 27, 27]             256\n","           Conv2d-31          [-1, 128, 27, 27]          36,992\n","        LeakyReLU-32          [-1, 128, 27, 27]               0\n","      BatchNorm2d-33          [-1, 128, 27, 27]             256\n","        FireStorm-34          [-1, 256, 27, 27]               0\n","           Conv2d-35           [-1, 32, 27, 27]           8,224\n","        LeakyReLU-36           [-1, 32, 27, 27]               0\n","      BatchNorm2d-37           [-1, 32, 27, 27]              64\n","           Conv2d-38          [-1, 128, 27, 27]           4,224\n","        LeakyReLU-39          [-1, 128, 27, 27]               0\n","      BatchNorm2d-40          [-1, 128, 27, 27]             256\n","           Conv2d-41          [-1, 128, 27, 27]          36,992\n","        LeakyReLU-42          [-1, 128, 27, 27]               0\n","      BatchNorm2d-43          [-1, 128, 27, 27]             256\n","        FireStorm-44          [-1, 256, 27, 27]               0\n","        MaxPool2d-45          [-1, 256, 13, 13]               0\n","           Conv2d-46           [-1, 48, 13, 13]          12,336\n","        LeakyReLU-47           [-1, 48, 13, 13]               0\n","      BatchNorm2d-48           [-1, 48, 13, 13]              96\n","           Conv2d-49          [-1, 192, 13, 13]           9,408\n","        LeakyReLU-50          [-1, 192, 13, 13]               0\n","      BatchNorm2d-51          [-1, 192, 13, 13]             384\n","           Conv2d-52          [-1, 192, 13, 13]          83,136\n","        LeakyReLU-53          [-1, 192, 13, 13]               0\n","      BatchNorm2d-54          [-1, 192, 13, 13]             384\n","        FireStorm-55          [-1, 384, 13, 13]               0\n","           Conv2d-56           [-1, 64, 13, 13]          24,640\n","        LeakyReLU-57           [-1, 64, 13, 13]               0\n","      BatchNorm2d-58           [-1, 64, 13, 13]             128\n","           Conv2d-59          [-1, 256, 13, 13]          16,640\n","        LeakyReLU-60          [-1, 256, 13, 13]               0\n","      BatchNorm2d-61          [-1, 256, 13, 13]             512\n","           Conv2d-62          [-1, 256, 13, 13]         147,712\n","        LeakyReLU-63          [-1, 256, 13, 13]               0\n","      BatchNorm2d-64          [-1, 256, 13, 13]             512\n","        FireStorm-65          [-1, 512, 13, 13]               0\n","AdaptiveAvgPool2d-66            [-1, 512, 1, 1]               0\n","          Flatten-67                  [-1, 512]               0\n","           Linear-68                  [-1, 512]         262,656\n","        LeakyReLU-69                  [-1, 512]               0\n","          Dropout-70                  [-1, 512]               0\n","           Linear-71                  [-1, 251]         128,763\n","================================================================\n","Total params: 809,451\n","Trainable params: 809,451\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 57.93\n","Params size (MB): 3.09\n","Estimated Total Size (MB): 61.60\n","----------------------------------------------------------------\n"]}],"source":["# Print the summary of the model\n","torchsummary.summary(StormModel, (3, 224, 224));"]},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2024-06-03T16:31:14.275625Z","start_time":"2024-06-03T16:30:54.879615Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.020478,"end_time":"2024-06-11T06:36:00.279534","exception":false,"start_time":"2024-06-11T06:36:00.259056","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Load data\n","if not is_kaggle():  # on kaggle the data must be already loaded with the correct structure\n","\n","    # check if the train folder is  already created\n","    if not os.path.exists('data/train'):\n","\n","        folder_structure = preprocessing.create_dataset()\n","\n","        # transform it in a dataframe and list the number of images per class in the folders\n","\n","        a = pd.DataFrame([(k, len(v)) for k,v in folder_structure[0].items()], \n","                            columns=['class', 'count'])\n","        b = pd.DataFrame([(k, len(v)) for k,v in folder_structure[1].items()], \n","                            columns=['class', 'count'])\n","        image_counts = pd.merge(a, \n","                                b, \n","                                on='class', \n","                                how='outer', \n","                                suffixes=('_train', '_test'))\n","        \n","        \n","        image_counts.loc[np.argmin(image_counts['count_train']),:]\n","        \n","        # create a validation set\n","        preprocessing.create_validation(42);"]},{"cell_type":"code","execution_count":6,"metadata":{"papermill":{"duration":0.017009,"end_time":"2024-06-11T06:36:01.306893","exception":false,"start_time":"2024-06-11T06:36:01.289884","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["'.'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Choose the correct data directory\n","if is_kaggle():\n","    im_dir = '/kaggle/input/food-dataset-sl/'\n","else:\n","    im_dir ='.'  \n","\n","im_dir"]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:08:44.782398Z","start_time":"2024-06-03T17:08:44.644528Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":55.710484,"end_time":"2024-06-11T06:36:57.054504","exception":false,"start_time":"2024-06-11T06:36:01.344020","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#Apply transformations to the images \n","transform = transforms.Compose([\n","    # resize \n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    #transforms.RandomCrop(224),\n","    transforms.ToTensor(),\n","    # Normalize pixel values\n","    transforms.Normalize(mean= [0.6388, 0.5445, 0.4448],  std =  [0.2713, 0.2864, 0.3131]),\n","])\n","\n","# set the num_worker for the dataloader\n","num_workers = 4\n","\n","# Load the training dataset\n","trainset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/train'), transform=transform)\n","\n","# Create data loader for training data\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=num_workers)\n","\n","valset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/val'), transform=transform)\n","\n","#create data loader for validation data\n","valloader = torch.utils.data.DataLoader(valset, batch_size=16, shuffle=True, num_workers=num_workers)\n","\n","testset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), transform=transform)\n","\n","#create data loader for test data\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=num_workers)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"papermill":{"duration":0.046366,"end_time":"2024-06-11T06:36:57.110796","exception":false,"start_time":"2024-06-11T06:36:57.064430","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Define criterion, optimizer and scheduler and other parameters for training\n","\n","#  PARAMETERS HERE ARE JUST EXAMPLES, THE PARAMETERS FOR THE USER ARE DEFINED IN THE FOLLOWING CELLS\n","\n","opt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\n","momentum = 0.9                      # momentum ONLY for SGD optimizer\n","weight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\n","step_size = 7                       # step size for the scheduler\n","gamma = 0.1                         # gamma for the scheduler\n","\n","batch_size = 8                      # batch size\n","num_epochs=10                       # number of epochs\n","patience = 3                        # patience for early stopping\n","criterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\"]\n","lr = 5e-5                           # learning rate\n","\n","model_name = \"StormModel\"           # model name\n","model = model                       # model\n","\n","\n","\n","#set the optimizer\n","if opt == \"Adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","elif opt == \"SGD\":\n","    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","else:\n","    print(\"Invalid optimizer\")\n","\n","#set the criterion\n","if criterion == \"CrossEntropyLoss\":\n","    criterion = nn.CrossEntropyLoss()\n","elif criterion == \"MSELoss\":\n","    criterion = nn.MSELoss()\n","else:\n","    print(\"Invalid criterion\")\n","\n","#set the scheduler\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","\n","# Upload model to correct device\n","model = model.to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"papermill":{"duration":0.021158,"end_time":"2024-06-11T06:36:57.141380","exception":false,"start_time":"2024-06-11T06:36:57.120222","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def set_training_parameters(model, model_name, opt, lr, weight_decay, momentum, criterion, step_size, gamma, num_epochs, patience, device):\n","\n","    #set the optimizer\n","    if opt == \"Adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    elif opt == \"SGD\":\n","        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","    else:\n","        print(\"Invalid optimizer\")\n","\n","    #set the criterion\n","    if criterion == \"CrossEntropyLoss\":\n","        criterion = nn.CrossEntropyLoss()\n","    elif criterion == \"MSELoss\":\n","        criterion = nn.MSELoss()\n","    elif criterion == \"L1Loss\":\n","        criterion = nn.L1Loss()\n","    elif criterion == \"NLLLoss\":\n","        criterion = nn.NLLLoss()\n","    else:\n","        print(\"Invalid criterion\")\n","\n","    #set the scheduler\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","\n","    # Upload model to correct device\n","    model = model.to(device)\n","    \n","    training_parameters = {}\n","    training_parameters['model'] = model\n","    training_parameters['model_name'] = model_name\n","    training_parameters['criterion'] = criterion\n","    training_parameters['optimizer'] = optimizer\n","    training_parameters['scheduler'] = scheduler\n","    training_parameters['num_epochs'] = num_epochs\n","    training_parameters['patience'] = patience\n","    \n","    return training_parameters"]},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2024-06-03T17:23:52.036996Z","start_time":"2024-06-03T17:20:54.527188Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.027131,"end_time":"2024-06-11T06:36:57.177754","exception":false,"start_time":"2024-06-11T06:36:57.150623","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Training function based on above parameters\n","def train_model(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs=10, patience=3 ):\n","\n","    mlflow.start_run(run_name=model_name)\n","\n","    # Log model parameters\n","    mlflow.log_param(\"optimizer\", opt)\n","    mlflow.log_param(\"learning_rate\", lr)\n","    mlflow.log_param(\"batch_size\", batch_size)\n","    mlflow.log_param(\"num_epochs\", num_epochs)\n","    mlflow.log_param(\"momentum\", momentum)\n","    mlflow.log_param(\"weight_decay\", weight_decay)\n","    mlflow.log_param(\"step_size\", step_size)\n","    mlflow.log_param(\"gamma\", gamma)\n","    mlflow.log_param(\"patience\", patience)\n","\n","    starting_step=0\n","\n","    patience_counter = 0\n","    best_model = None\n","    best_loss = np.inf\n","        \n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","        running_loss = 0.0\n","        train_loader_tqdm = tqdm.tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", \n","                                unit=\"batch\")\n","        ind_rloss=1\n","        for inputs, labels in train_loader_tqdm:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Print statistics\n","            running_loss += loss.item()\n","            train_loader_tqdm.set_postfix(loss=running_loss / ind_rloss)\n","            ind_rloss +=1\n","\n","        epoch_loss = running_loss / len(trainloader)\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n","\n","        scheduler.step()\n","        mlflow.log_metric(\"train_loss\", epoch_loss, step=starting_step+epoch)\n","        \n","        # Validation loop (optional)\n","        model.eval()  # Set model to evaluation mode\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for inputs, labels in valloader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss /= len(valloader)\n","        val_accuracy = 100 * correct / total\n","        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n","        \n","        # Log validation loss and accuracy\n","        mlflow.log_metric(\"val_loss\", val_loss, step=starting_step+epoch)\n","        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=starting_step+epoch)\n","        \n","        # Early stopping\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            best_model = model\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter > patience:\n","                print(\"Early stopping\")\n","                break\n","                    \n","    # Log the model\n","    mlflow.pytorch.log_model(best_model, model_name)\n","\n","    # End the MLflow run\n","    mlflow.end_run()\n","\n","    print('Finished Training')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"papermill":{"duration":6330.346339,"end_time":"2024-06-11T08:22:27.579773","exception":false,"start_time":"2024-06-11T06:36:57.233434","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/10:   3%|▎         | 342/11860 [00:07<04:15, 45.04batch/s, loss=5.52]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#stop eventual mlflow runs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtr_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, model_name, trainloader, valloader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m train_loader_tqdm\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mrunning_loss \u001b[38;5;241m/\u001b[39m ind_rloss)\n\u001b[1;32m     46\u001b[0m ind_rloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#Training section: MODIFY THE PARAMETERS BELOW\n","\n","# Define criterion, optimizer and scheduler and other parameters for training\n","opt = \"Adam\"                        # optimizer to be used: [\"Adam\" or \"SGD\"]\n","momentum = 0.9                      # momentum ONLY for SGD optimizer\n","weight_decay = 1e-4                 # weight decay ONLY on Adam optimizer\n","step_size = 20                     # step size for the scheduler\n","gamma = 0.5                         # gamma for the scheduler\n","\n","batch_size = 64                      # batch size\n","num_epochs= 10                     # number of epochs\n","patience = 5                        # patience for early stopping\n","criterion =\"CrossEntropyLoss\"       # loss function to be used: [\"CrossEntropyLoss\", \"MSELoss\", \"L1Loss\", \"NLLLoss\"]\n","lr = 1e-5                           # learning rate\n","\n","model_name = \"SSL_Storm_Model_samedata\"           # model name\n","model =  StormModel               # model\n","\n","tr_param = set_training_parameters(model=model,model_name = model_name, opt=opt, lr=lr, weight_decay=weight_decay, \n","                                   momentum=momentum, criterion=criterion, step_size=step_size, gamma=gamma, num_epochs=num_epochs, \n","                                   patience=patience, device=device)\n","\n","#stop eventual mlflow runs\n","mlflow.end_run()\n","train_model(**tr_param, trainloader=trainloader, valloader=valloader)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5175452,"sourceId":8646217,"sourceType":"datasetVersion"},{"sourceId":182257654,"sourceType":"kernelVersion"},{"sourceId":182257811,"sourceType":"kernelVersion"},{"sourceId":182257893,"sourceType":"kernelVersion"},{"sourceId":183300643,"sourceType":"kernelVersion"},{"isSourceIdPinned":true,"modelInstanceId":53173,"sourceId":63760,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":53503,"sourceId":64167,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":53887,"sourceId":64611,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":54198,"sourceId":64968,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":54229,"sourceId":65006,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":54311,"sourceId":65097,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":54317,"sourceId":65103,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"papermill":{"default_parameters":{},"duration":6511.917668,"end_time":"2024-06-11T08:23:45.601748","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-11T06:35:13.684080","version":"2.5.0"}},"nbformat":4,"nbformat_minor":4}
