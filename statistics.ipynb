{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import scripts.models as models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import scripts.utils as utils\n",
    "import scripts.preprocessing as preprocessing\n",
    "import os\n",
    "import torchvision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common properties for a scientific paper with two-column format for the plots\n",
    "plt.rcParams.update({\n",
    "    'font.size': 8,              # Global font size\n",
    "    'axes.titlesize': 10,        # Title font size\n",
    "    'axes.labelsize': 8,         # Axes labels font size\n",
    "    'xtick.labelsize': 8,        # X-tick labels font size\n",
    "    'ytick.labelsize': 8,        # Y-tick labels font size\n",
    "    'legend.fontsize': 8,        # Legend font size\n",
    "    'figure.titlesize': 10,      # Figure title font size\n",
    "    'lines.linewidth': 1.0,      # Line width\n",
    "    'axes.linewidth': 0.8,       # Axes border width\n",
    "    'xtick.major.width': 0.8,    # Major tick width\n",
    "    'ytick.major.width': 0.8,    # Major tick width\n",
    "    'xtick.minor.width': 0.6,    # Minor tick width\n",
    "    'ytick.minor.width': 0.6,    # Minor tick width\n",
    "    'xtick.major.size': 3,       # Major tick size\n",
    "    'ytick.major.size': 3,       # Major tick size\n",
    "    'xtick.minor.size': 2,       # Minor tick size\n",
    "    'ytick.minor.size': 2,       # Minor tick size\n",
    "    'legend.loc': 'best',        # Legend location\n",
    "    'savefig.dpi': 300,          # Save figure resolution\n",
    "    'figure.figsize': [7.5, 3.5] # Figure size in inches (adjust as needed)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "device = utils.use_GPU()\n",
    "model = models.StormModel()\n",
    "state_dict = torch.load('final_models/model_50_clean.pth') \n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations for the test data\n",
    "transform = transforms.Compose([\n",
    "    # resize \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize pixel values, means are calculated on the food151 dataset\n",
    "    transforms.Normalize(mean= [0.6388, 0.5445, 0.4448],  std =  [0.2713, 0.2864, 0.3131]),\n",
    "])\n",
    "\n",
    "num_workers = 4\n",
    "im_dir = '.'\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), \n",
    "                                           transform =transform\n",
    "                                           )\n",
    "plotset = torchvision.datasets.ImageFolder(root=os.path.join(im_dir,'data/test'), \n",
    "                                           transform = transforms.Compose([\n",
    "                                                transforms.Resize((224, 224)),\n",
    "                                                transforms.ToTensor()])\n",
    "                                                )\n",
    "\n",
    "#create data loader for test data and for plot of images without normalization\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, \n",
    "                                         shuffle=False, \n",
    "                                         num_workers=num_workers)\n",
    "plotloader = torch.utils.data.DataLoader(plotset, batch_size=1, \n",
    "                                         shuffle=False, \n",
    "                                         num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set and print some statistics like recall and precision and confusion matrix\n",
    "predictions = utils.get_all_predictions(model, testloader)\n",
    "predicted_labels = predictions.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the true labels\n",
    "true_labels = torch.tensor([label for image, label in testset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = torch.sum(true_labels == predicted_labels)/len(testset)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels.cpu().numpy(), predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute precision and recall for each class\n",
    "precision = precision_score(true_labels.cpu().numpy(), predicted_labels.cpu().numpy(), average=None, zero_division=0)\n",
    "recall = recall_score(true_labels.cpu().numpy(), predicted_labels.cpu().numpy(), average=None, zero_division=0)\n",
    "\n",
    "mean_precision = np.mean(precision)\n",
    "mean_recall = np.mean(recall)\n",
    "\n",
    "# plot the results for precision\n",
    "fig = plt.figure()\n",
    "plt.bar(range(len(precision)),precision, width=0.9)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision per class')\n",
    "plt.grid(axis=\"y\",alpha=0.3)\n",
    "plt.hlines(mean_precision, 0, len(precision), colors='r', linestyles='dashed', label=f'Mean precision = {mean_precision:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig.savefig('../figures_report/precision.jpg')\n",
    "\n",
    "# plot the results for recall\n",
    "fig = plt.figure()\n",
    "plt.bar(np.arange(len(recall)),recall, width=0.9, color='orange')\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall per class')\n",
    "plt.grid(axis=\"y\",alpha=0.3)\n",
    "plt.hlines(mean_recall, 0, len(recall), colors='r', linestyles='dashed', label=f'Mean recall = {mean_recall:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('../figures_report/recall.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_precision = np.where(precision == 0)[0]\n",
    "print(\"Classes with 0 precision:\")\n",
    "for i in zero_precision:\n",
    "    print(classes[i]) \n",
    "\n",
    "print()\n",
    "\n",
    "sorted_precision = np.argsort(precision)\n",
    "\n",
    "# remove the classes with 0 precision, take the last and the first 5\n",
    "sorted_precision_no_zeros = sorted_precision[precision[sorted_precision] != 0]\n",
    "\n",
    "top_5_precision = sorted_precision[-5:]\n",
    "\n",
    "bottom_5_precision = sorted_precision_no_zeros[:5]\n",
    "\n",
    "#reorder top_5 because they are inverted\n",
    "top_5_precision = top_5_precision[::-1]\n",
    "\n",
    "print(\"Top 5 precision classes\")\n",
    "for i in top_5_precision:\n",
    "    print(classes[i], precision[i])\n",
    "print()\n",
    "print(\"Bottom 5 precision classes\")\n",
    "for i in bottom_5_precision:\n",
    "    print(classes[i], precision[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, best case\n",
    "print(\"Recall for the top 5 precision classes\")\n",
    "for i in top_5_precision:\n",
    "    print(classes[i], recall[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall, worst case\n",
    "print(\"Recall for the bottom 5 precision classes\")\n",
    "for i in bottom_5_precision:\n",
    "    print(classes[i], recall[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall for the classes with 0 precision (we expect 0 recall)\n",
    "print(\"Recall for the classes with 0 precision\")\n",
    "for i in zero_precision:\n",
    "    print(classes[i], recall[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the class with 0 precison and plot for the first one the classes that are predicted instead of it\n",
    "# get the index of the class with 0 precision\n",
    "index = zero_precision[3]\n",
    "import seaborn as sns\n",
    "# detect the column in the confusion matrix\n",
    "col = cm[:, index]\n",
    "\n",
    "\n",
    "\n",
    "#save the classes with a value higher than zero\n",
    "predicted_classes = np.where(col > 0)[0]\n",
    "sns.color_palette('Set2')\n",
    "#plot it \n",
    "fig = plt.figure(figsize=(7,4))\n",
    "plt.barh(np.arange(len(predicted_classes)), col[predicted_classes])\n",
    "plt.yticks(np.arange(len(predicted_classes)), [classes[i] for i in predicted_classes])\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('Class')\n",
    "plt.title(f'Predictions for class \\\"{classes[index]}\\\"')\n",
    "plt.grid(axis=\"x\",alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/predictions_{classes[index]}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider now the recall for the same class\n",
    "\n",
    "import seaborn as sns\n",
    "# detect the line in the confusion matrix\n",
    "line = cm[index]\n",
    "\n",
    "#save the classes with a value higher than zero\n",
    "predicted_classes = np.where(line > 0)[0]\n",
    "sns.color_palette('Set2')\n",
    "#plot it \n",
    "fig = plt.figure(figsize=(7,4))\n",
    "plt.barh(np.arange(len(predicted_classes)), line[predicted_classes])\n",
    "plt.yticks(np.arange(len(predicted_classes)), [classes[i] for i in predicted_classes])\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('Class')\n",
    "plt.title(f'Predictions for class \\\"{classes[index]}\\\"')\n",
    "plt.grid(axis=\"x\",alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/predictions_{classes[index]}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a image of th class \"index\" from the test set and one from the ones that are predicted instead of it\n",
    "\n",
    "max_pred = np.argmax(line)\n",
    "\n",
    "# get the first image of the class \"index\"\n",
    "for i, (image, label) in enumerate(plotloader):\n",
    "    if label == index:\n",
    "        break\n",
    "\n",
    "image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'Class: {classes[index]}')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/images_{classes[index]}.jpg')\n",
    "\n",
    "# get image for the most predicted class\n",
    "for i, (image, label) in enumerate(plotloader):\n",
    "    if label == max_pred:\n",
    "        break\n",
    "\n",
    "image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'Class: {classes[max_pred]}')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/images_{classes[index]}_{classes[max_pred]}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider now the precision for the class \"savarin\"\n",
    "\n",
    "import seaborn as sns\n",
    "index = top_5_precision[1]\n",
    "# detect the line in the confusion matrix\n",
    "line = cm[:,index]\n",
    "\n",
    "#save the classes with a value higher than zero\n",
    "predicted_classes = np.where(line > 0)[0]\n",
    "sns.color_palette('Set2')\n",
    "#plot it \n",
    "fig = plt.figure(figsize=(7,4))\n",
    "plt.barh(np.arange(len(predicted_classes)), line[predicted_classes])\n",
    "plt.yticks(np.arange(len(predicted_classes)), [classes[i] for i in predicted_classes])\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('Class')\n",
    "plt.title(f'Predictions for class \\\"{classes[index]}\\\"')\n",
    "plt.grid(axis=\"x\",alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/predictions_{classes[index]}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a image of th class \"index\" from the test set and one from the ones that are predicted instead of it\n",
    "\n",
    "max_pred = np.argmax(line)\n",
    "\n",
    "# get the first image of the class \"index\"\n",
    "for i, (image, label) in enumerate(plotloader):\n",
    "    if label == index:\n",
    "        break\n",
    "\n",
    "image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'Class: {classes[index]}')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/images_{classes[index]}.jpg')\n",
    "\n",
    "# get image for the most predicted class\n",
    "for i, (image, label) in enumerate(plotloader):\n",
    "    if label == max_pred:\n",
    "        break\n",
    "\n",
    "image = image.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'Class: {classes[max_pred]}')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'../figures_report/images_{classes[index]}_{classes[max_pred]}.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
